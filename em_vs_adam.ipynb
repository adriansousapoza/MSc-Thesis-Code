{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam vs EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "import utils.gmm\n",
    "import utils.gmm_adam\n",
    "import utils.metrics\n",
    "import utils.priors\n",
    "importlib.reload(utils.gmm)\n",
    "importlib.reload(utils.gmm_adam)\n",
    "importlib.reload(utils.metrics)\n",
    "importlib.reload(utils.priors)\n",
    "from utils.metrics import ClusteringMetrics\n",
    "from utils.gmm import GaussianMixture\n",
    "from utils.gmm_adam import GaussianMixtureAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "init_params = 'kmeans'\n",
    "n_features = 4\n",
    "n_components = 4\n",
    "max_iter = 1001\n",
    "covariance_type = 'full'\n",
    "reg_covar = 1e-4\n",
    "tol = 1e-4\n",
    "\n",
    "# Updated data creation for 4 dimensions\n",
    "n_samples_1 = 1000\n",
    "n_samples_2 = 800\n",
    "n_samples_3 = 400\n",
    "n_samples_4 = 600\n",
    "\n",
    "# Center coordinates in a 4D space\n",
    "center_1 = np.array([0, 0, 0, 0])\n",
    "center_2 = np.array([-4, 4, -4, 4])\n",
    "center_3 = np.array([4, -4, 4, -4])\n",
    "center_4 = np.array([4, 4, 4, 4])\n",
    "\n",
    "# Generating 4-dimensional datasets\n",
    "np.random.seed(0)\n",
    "C_1 = np.random.rand(4, 4)  # Random transformation matrix for component 1\n",
    "C_2 = np.random.rand(4, 4)  # Random transformation matrix for component 2\n",
    "\n",
    "component_1 = np.dot(np.random.randn(n_samples_1, 4), C_1) + center_1\n",
    "component_2 = 0.7 * np.random.randn(n_samples_2, 4) + center_2\n",
    "component_3 = .5 * np.random.randn(n_samples_3, 4) + center_3\n",
    "component_4 = np.dot(np.random.randn(n_samples_4, 4), C_2) + center_4\n",
    "\n",
    "X = np.concatenate([component_1, component_2, component_3, component_4])\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.cat([torch.zeros(n_samples_1), torch.ones(n_samples_2), 2 * torch.ones(n_samples_3), 3 * torch.ones(n_samples_4)]).long()\n",
    "\n",
    "centers_4d = [np.array([0, 0, 0, 0]), np.array([-4, 4, -4, 4]), np.array([4, -4, 4, -4]), np.array([4, 4, 4, 4])]\n",
    "\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM-based GMM Results: {'rand_score': 0.9963458776473999, 'adjusted_rand_score': 0.990842342376709, 'mutual_info_score': 1.3139123916625977, 'adjusted_mutual_info_score': 0.9851972460746765, 'normalized_mutual_info_score': 0.98520427942276, 'fowlkes_mallows_score': 0.9933634996414185, 'homogeneity_score': 0.9851346015930176, 'completeness_score': 0.9852737784385681, 'v_measure_score': 0.9852041851005188, 'purity_score': 0.9967857003211975, 'classification_report': {0: {'precision': 0.00667779632721202, 'recall': 0.004, 'f1-score': 0.0050031269543464665, 'support': 1000, 'jaccard': 0.0025078369905956114, 'roc_auc': 0.1655922532081604}, 1: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 800, 'jaccard': 0.0, 'roc_auc': 0.03547938913106918}, 2: {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 400, 'jaccard': 1.0, 'roc_auc': 1.0}, 3: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 600, 'jaccard': 0.0, 'roc_auc': 0.5632045269012451}}, 'confusion_matrix': tensor([[  4, 996,   0,   0],\n",
      "        [  0,   0,   0, 800],\n",
      "        [  0,   0, 400,   0],\n",
      "        [595,   5,   0,   0]], dtype=torch.int32), 'silhouette_score': 0.6674441695213318, 'davies_bouldin_index': 0.47364187240600586, 'calinski_harabasz_score': 7822.94873046875, 'bic_score': 28528.8984375, 'aic_score': 28178.594177246094, 'dunn_index': tensor(0.0179)}\n",
      "Adam-optimized GMM Results: {'rand_score': 0.9911407232284546, 'adjusted_rand_score': 0.9777509570121765, 'mutual_info_score': 1.297065258026123, 'adjusted_mutual_info_score': 0.971351683139801, 'normalized_mutual_info_score': 0.9713653922080994, 'fowlkes_mallows_score': 0.983860433101654, 'homogeneity_score': 0.972569465637207, 'completeness_score': 0.9701581001281738, 'v_measure_score': 0.9713622863569499, 'purity_score': 0.9921428561210632, 'classification_report': {0: {'precision': 0.032362459546925564, 'recall': 0.02, 'f1-score': 0.024721878862793575, 'support': 1000, 'jaccard': 0.012515644555694618, 'roc_auc': 0.1814688891172409}, 1: {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 800, 'jaccard': 1.0, 'roc_auc': 1.0}, 2: {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 400, 'jaccard': 1.0, 'roc_auc': 1.0}, 3: {'precision': 0.002036659877800407, 'recall': 0.0033333333333333335, 'f1-score': 0.0025284450063211123, 'support': 600, 'jaccard': 0.0012658227848101266, 'roc_auc': 0.44116440415382385}}, 'confusion_matrix': tensor([[ 20,   0,   0, 980],\n",
      "        [  0, 800,   0,   0],\n",
      "        [  0,   0, 400,   0],\n",
      "        [598,   0,   0,   2]], dtype=torch.int32), 'silhouette_score': 0.6703314781188965, 'davies_bouldin_index': 0.4701383709907532, 'calinski_harabasz_score': 7966.8095703125, 'bic_score': 32357.212890625, 'aic_score': 32006.908767700195, 'dunn_index': tensor(0.0171)}\n"
     ]
    }
   ],
   "source": [
    "n_features = X_tensor.shape[1]\n",
    "\n",
    "# Initialize the GMM optimized with Adam\n",
    "gmm_adam = GaussianMixtureAdam(\n",
    "    n_features=n_features,\n",
    "    n_components=n_components,\n",
    "    covariance_type=covariance_type,\n",
    "    max_iter=max_iter,\n",
    "    learning_rate=1e-1,\n",
    "    init_params=init_params,\n",
    "    reg_covar=reg_covar,\n",
    ")\n",
    "\n",
    "# Initialize the EM-based GMM\n",
    "gmm_em = GaussianMixture(\n",
    "    n_features=n_features,\n",
    "    n_components=n_components,\n",
    "    covariance_type=covariance_type,\n",
    "    max_iter=max_iter,\n",
    "    init_params=init_params,\n",
    "    reg_covar=reg_covar,\n",
    ")\n",
    "\n",
    "# Fit the EM-based GMM\n",
    "gmm_em.fit(X_tensor)\n",
    "\n",
    "# Predict and evaluate\n",
    "labels_em = gmm_em.predict(X_tensor)\n",
    "results_em = gmm_em.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "print(\"EM-based GMM Results:\", results_em)\n",
    "\n",
    "# Fit the Adam-optimized GMM\n",
    "gmm_adam.fit(X_tensor)\n",
    "\n",
    "# Predict and evaluate\n",
    "labels_adam = gmm_adam.predict(X_tensor)\n",
    "results_adam = gmm_adam.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "print(\"Adam-optimized GMM Results:\", results_adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic data\n",
    "def generate_data(n_samples, n_features, centers, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    X = []\n",
    "    for center in centers:\n",
    "        center_adjusted = np.pad(center, (0, max(0, n_features - len(center))), mode='constant')\n",
    "        X.append(np.random.randn(n_samples // len(centers), n_features) + center_adjusted[:n_features])  # Divide samples equally\n",
    "    X = np.concatenate(X)\n",
    "    return torch.tensor(X, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a single experiment (comparing EM and Adam-based GMM with different learning rates)\n",
    "def run_experiment(X_tensor, y_tensor, dim, size, n_components, experiment_name, learning_rates=[1e-1, 1e-2, 1e-3]):\n",
    "    results = []\n",
    "    print(f\"\\n Running {experiment_name} with {dim} dimensions, with {n_components} clusters and {size} samples\")\n",
    "    \n",
    "    # Time the EM-based GMM\n",
    "    start_time = time.time()  # Start the timer\n",
    "    gmm_em = GaussianMixture(\n",
    "        n_features=dim,\n",
    "        n_components=n_components,\n",
    "        covariance_type=covariance_type,\n",
    "        max_iter=max_iter,\n",
    "        init_params=init_params,\n",
    "        reg_covar=reg_covar,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    gmm_em.fit(X_tensor)\n",
    "    end_time = time.time()  # End the timer\n",
    "    time_taken_em = end_time - start_time  # Calculate the time difference\n",
    "\n",
    "    results_em = gmm_em.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "    print(f\"EM converged in {gmm_em.n_iter_} iterations with log-likelihood: {gmm_em.lower_bound_}, Time taken: {time_taken_em:.4f} seconds\")\n",
    "\n",
    "    # Collect EM results\n",
    "    results.append({\n",
    "        'experiment': experiment_name,\n",
    "        'method': 'EM',\n",
    "        'n_features': dim,\n",
    "        'n_clusters': n_components,\n",
    "        'data_size': size,\n",
    "        'learning_rate': None,\n",
    "        'iterations': gmm_em.n_iter_,\n",
    "        'log_likelihood': gmm_em.lower_bound_,\n",
    "        'time_taken': time_taken_em,  # Add time to the results\n",
    "        **results_em\n",
    "    })\n",
    "    \n",
    "    # Run Adam-based GMM with different learning rates\n",
    "    for lr in learning_rates:\n",
    "        start_time = time.time()  # Start the timer\n",
    "        gmm_adam = GaussianMixtureAdam(\n",
    "            n_features=dim,\n",
    "            n_components=n_components,\n",
    "            covariance_type=covariance_type,\n",
    "            max_iter=max_iter,\n",
    "            learning_rate=lr,\n",
    "            init_params=init_params,\n",
    "            reg_covar=reg_covar,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        gmm_adam.fit(X_tensor)\n",
    "        end_time = time.time()  # End the timer\n",
    "        time_taken_adam = end_time - start_time  # Calculate the time difference\n",
    "\n",
    "        results_adam = gmm_adam.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "        print(f\"Adam (lr={lr}) converged in {gmm_adam.n_iter_} iterations with log-likelihood: {gmm_adam.lower_bound_}, Time taken: {time_taken_adam:.4f} seconds\")\n",
    "        \n",
    "        # Collect Adam results for each learning rate\n",
    "        results.append({\n",
    "            'experiment': experiment_name,\n",
    "            'method': 'Adam',\n",
    "            'n_features': dim,\n",
    "            'n_clusters': n_components,\n",
    "            'data_size': size,\n",
    "            'learning_rate': lr,\n",
    "            'iterations': gmm_adam.n_iter_,\n",
    "            'log_likelihood': gmm_adam.lower_bound_,\n",
    "            'time_taken': time_taken_adam,  # Add time to the results\n",
    "            **results_adam\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment 1: Well-separated Clusters, Increasing Dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Experiment 1 with 2 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 8 iterations with log-likelihood: -4.162824630737305, Time taken: 0.0547 seconds\n",
      "Adam (lr=0.1) converged in 54 iterations with log-likelihood: -4.162992477416992, Time taken: 1.1293 seconds\n",
      "Adam (lr=0.01) converged in 20 iterations with log-likelihood: -4.163053035736084, Time taken: 0.4838 seconds\n",
      "Adam (lr=0.001) converged in 184 iterations with log-likelihood: -4.163110733032227, Time taken: 2.9411 seconds\n",
      "\n",
      " Running Experiment 1 with 4 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.963935375213623, Time taken: 0.0239 seconds\n",
      "Adam (lr=0.1) converged in 28 iterations with log-likelihood: -6.96526575088501, Time taken: 0.6280 seconds\n",
      "Adam (lr=0.01) converged in 14 iterations with log-likelihood: -6.964781761169434, Time taken: 0.3228 seconds\n",
      "Adam (lr=0.001) converged in 152 iterations with log-likelihood: -6.964130401611328, Time taken: 2.2223 seconds\n",
      "\n",
      " Running Experiment 1 with 8 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -12.549822807312012, Time taken: 0.0363 seconds\n",
      "Adam (lr=0.1) converged in 47 iterations with log-likelihood: -12.550396919250488, Time taken: 0.6299 seconds\n",
      "Adam (lr=0.01) converged in 49 iterations with log-likelihood: -12.549958229064941, Time taken: 1.0638 seconds\n",
      "Adam (lr=0.001) converged in 192 iterations with log-likelihood: -12.550300598144531, Time taken: 2.7765 seconds\n",
      "\n",
      " Running Experiment 1 with 16 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -23.618139266967773, Time taken: 0.0272 seconds\n",
      "Adam (lr=0.1) converged in 67 iterations with log-likelihood: -23.618886947631836, Time taken: 1.0280 seconds\n",
      "Adam (lr=0.01) converged in 59 iterations with log-likelihood: -23.618337631225586, Time taken: 1.3584 seconds\n",
      "Adam (lr=0.001) converged in 231 iterations with log-likelihood: -23.619237899780273, Time taken: 4.5799 seconds\n",
      "\n",
      " Running Experiment 1 with 32 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -45.4277458190918, Time taken: 0.0810 seconds\n",
      "Adam (lr=0.1) converged in 1000 iterations with log-likelihood: -46.352787017822266, Time taken: 23.5225 seconds\n",
      "Adam (lr=0.01) converged in 67 iterations with log-likelihood: -45.42816925048828, Time taken: 1.6143 seconds\n",
      "Adam (lr=0.001) converged in 262 iterations with log-likelihood: -45.430049896240234, Time taken: 6.2071 seconds\n",
      "\n",
      " Running Experiment 1 with 64 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 5 iterations with log-likelihood: -87.16508483886719, Time taken: 0.1819 seconds\n",
      "Adam (lr=0.1) converged in 1000 iterations with log-likelihood: -119.89878845214844, Time taken: 34.7332 seconds\n",
      "Adam (lr=0.01) converged in 1000 iterations with log-likelihood: -88.35476684570312, Time taken: 34.2919 seconds\n",
      "Adam (lr=0.001) converged in 292 iterations with log-likelihood: -91.98446655273438, Time taken: 10.6775 seconds\n",
      "\n",
      " Running Experiment 1 with 128 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 3 iterations with log-likelihood: -161.88819885253906, Time taken: 0.3574 seconds\n",
      "Adam (lr=0.1) converged in 1000 iterations with log-likelihood: -224.79910278320312, Time taken: 77.3025 seconds\n",
      "Adam (lr=0.01) converged in 214 iterations with log-likelihood: -163.68370056152344, Time taken: 16.1912 seconds\n",
      "Adam (lr=0.001) converged in 379 iterations with log-likelihood: -161.8994140625, Time taken: 28.3954 seconds\n",
      "     experiment method  n_features  n_clusters  data_size  learning_rate  \\\n",
      "0  Experiment 1     EM           2           4       1000            NaN   \n",
      "1  Experiment 1   Adam           2           4       1000          0.100   \n",
      "2  Experiment 1   Adam           2           4       1000          0.010   \n",
      "3  Experiment 1   Adam           2           4       1000          0.001   \n",
      "0  Experiment 1     EM           4           4       1000            NaN   \n",
      "1  Experiment 1   Adam           4           4       1000          0.100   \n",
      "2  Experiment 1   Adam           4           4       1000          0.010   \n",
      "3  Experiment 1   Adam           4           4       1000          0.001   \n",
      "0  Experiment 1     EM           8           4       1000            NaN   \n",
      "1  Experiment 1   Adam           8           4       1000          0.100   \n",
      "2  Experiment 1   Adam           8           4       1000          0.010   \n",
      "3  Experiment 1   Adam           8           4       1000          0.001   \n",
      "0  Experiment 1     EM          16           4       1000            NaN   \n",
      "1  Experiment 1   Adam          16           4       1000          0.100   \n",
      "2  Experiment 1   Adam          16           4       1000          0.010   \n",
      "3  Experiment 1   Adam          16           4       1000          0.001   \n",
      "0  Experiment 1     EM          32           4       1000            NaN   \n",
      "1  Experiment 1   Adam          32           4       1000          0.100   \n",
      "2  Experiment 1   Adam          32           4       1000          0.010   \n",
      "3  Experiment 1   Adam          32           4       1000          0.001   \n",
      "0  Experiment 1     EM          64           4       1000            NaN   \n",
      "1  Experiment 1   Adam          64           4       1000          0.100   \n",
      "2  Experiment 1   Adam          64           4       1000          0.010   \n",
      "3  Experiment 1   Adam          64           4       1000          0.001   \n",
      "0  Experiment 1     EM         128           4       1000            NaN   \n",
      "1  Experiment 1   Adam         128           4       1000          0.100   \n",
      "2  Experiment 1   Adam         128           4       1000          0.010   \n",
      "3  Experiment 1   Adam         128           4       1000          0.001   \n",
      "\n",
      "   iterations  log_likelihood  time_taken  rand_score  ...  v_measure_score  \\\n",
      "0           8       -4.162825    0.054718    0.992006  ...         0.968715   \n",
      "1          54       -4.162992    1.129257    0.993019  ...         0.973429   \n",
      "2          20       -4.163053    0.483818    0.992006  ...         0.968715   \n",
      "3         184       -4.163111    2.941135    0.995009  ...         0.979847   \n",
      "0           2       -6.963935    0.023888    1.000000  ...         1.000000   \n",
      "1          28       -6.965266    0.627969    1.000000  ...         1.000000   \n",
      "2          14       -6.964782    0.322751    1.000000  ...         1.000000   \n",
      "3         152       -6.964130    2.222317    1.000000  ...         1.000000   \n",
      "0           2      -12.549823    0.036266    1.000000  ...         1.000000   \n",
      "1          47      -12.550397    0.629909    1.000000  ...         1.000000   \n",
      "2          49      -12.549958    1.063838    1.000000  ...         1.000000   \n",
      "3         192      -12.550301    2.776491    1.000000  ...         1.000000   \n",
      "0           2      -23.618139    0.027191    1.000000  ...         1.000000   \n",
      "1          67      -23.618887    1.027983    1.000000  ...         1.000000   \n",
      "2          59      -23.618338    1.358350    1.000000  ...         1.000000   \n",
      "3         231      -23.619238    4.579856    1.000000  ...         1.000000   \n",
      "0           2      -45.427746    0.081036    1.000000  ...         1.000000   \n",
      "1        1000      -46.352787   23.522479    1.000000  ...         1.000000   \n",
      "2          67      -45.428169    1.614253    1.000000  ...         1.000000   \n",
      "3         262      -45.430050    6.207142    1.000000  ...         1.000000   \n",
      "0           5      -87.165085    0.181937    0.846330  ...         0.783287   \n",
      "1        1000     -119.898788   34.733240    0.778176  ...         0.509648   \n",
      "2        1000      -88.354767   34.291880    0.845459  ...         0.782808   \n",
      "3         292      -91.984467   10.677482    0.845139  ...         0.782630   \n",
      "0           3     -161.888199    0.357382    1.000000  ...         1.000000   \n",
      "1        1000     -224.799103   77.302456    0.826665  ...              NaN   \n",
      "2         214     -163.683701   16.191163    1.000000  ...         1.000000   \n",
      "3         379     -161.899414   28.395369    1.000000  ...         1.000000   \n",
      "\n",
      "   purity_score                              classification_report  \\\n",
      "0         0.992  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1         0.993  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2         0.992  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3         0.995  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0         1.000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "1         1.000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "2         1.000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "3         1.000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "0         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0         0.749  {0: {'precision': 1.0, 'recall': 0.996, 'f1-sc...   \n",
      "1         0.670  {0: {'precision': 0.0945945945945946, 'recall'...   \n",
      "2         0.749  {0: {'precision': 1.0, 'recall': 0.996, 'f1-sc...   \n",
      "3         0.749  {0: {'precision': 1.0, 'recall': 0.996, 'f1-sc...   \n",
      "0         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1         0.721  {0: {'precision': 0.07392996108949416, 'recall...   \n",
      "2         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3         1.000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "\n",
      "                                    confusion_matrix  silhouette_score  \\\n",
      "0  [[tensor(0, dtype=torch.int32), tensor(250, dt...          0.676292   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(250, dt...          0.676438   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(250, dt...          0.676292   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(250, dt...          0.677050   \n",
      "0  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "1  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "2  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "3  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.553139   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.553139   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.553139   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.553139   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.415637   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.415637   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.415637   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.415637   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.282642   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.282642   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.282642   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.282642   \n",
      "0  [[tensor(249, dtype=torch.int32), tensor(0, dt...          0.051665   \n",
      "1  [[tensor(49, dtype=torch.int32), tensor(11, dt...          0.040030   \n",
      "2  [[tensor(249, dtype=torch.int32), tensor(0, dt...          0.050969   \n",
      "3  [[tensor(249, dtype=torch.int32), tensor(0, dt...          0.051101   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.099790   \n",
      "1  [[tensor(19, dtype=torch.int32), tensor(25, dt...               NaN   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.099790   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.099790   \n",
      "\n",
      "   davies_bouldin_index  calinski_harabasz_score      bic_score  \\\n",
      "0              0.438264              3878.841309    8484.527344   \n",
      "1              0.437710              3879.885498    8484.863281   \n",
      "2              0.438264              3878.840576    8484.984375   \n",
      "3              0.435614              3892.601318    8485.099609   \n",
      "0              0.463784              3787.711426   14335.428711   \n",
      "1              0.463784              3787.711182   14338.088867   \n",
      "2              0.463784              3787.711182   14337.121094   \n",
      "3              0.463784              3787.711182   14335.818359   \n",
      "0              0.670849              1905.694336   26336.134766   \n",
      "1              0.670849              1905.693726   26337.281250   \n",
      "2              0.670849              1905.693726   26336.404297   \n",
      "3              0.670849              1905.693726   26337.089844   \n",
      "0              0.970673               935.845825   51456.914062   \n",
      "1              0.970673               935.845642   51458.414062   \n",
      "2              0.970673               935.845642   51457.312500   \n",
      "3              0.970673               935.845642   51459.117188   \n",
      "0              1.394211               463.646484  106349.585938   \n",
      "1              1.394211               463.646484  108199.664062   \n",
      "2              1.394211               463.646484  106350.429688   \n",
      "3              1.394211               463.646484  106354.195312   \n",
      "0              4.738781               119.284065  233591.812500   \n",
      "1              4.717766               102.063843  299059.218750   \n",
      "2              4.842249               119.222481  235971.156250   \n",
      "3              4.837813               119.249062  243230.562500   \n",
      "0              2.791141               115.523918  555455.625000   \n",
      "1                   NaN                      NaN  681277.437500   \n",
      "2              2.791141               115.523918  559046.625000   \n",
      "3              2.791141               115.523918  555478.000000   \n",
      "\n",
      "       aic_score                       dunn_index  \n",
      "0    8371.649261  tensor(0.0559, device='cuda:0')  \n",
      "1    8371.984955                   tensor(0.0559)  \n",
      "2    8372.106071                   tensor(0.0559)  \n",
      "3    8372.221466                   tensor(0.0577)  \n",
      "0   14045.870750  tensor(0.3219, device='cuda:0')  \n",
      "1   14048.531502                   tensor(0.3219)  \n",
      "2   14047.563522                   tensor(0.3219)  \n",
      "3   14046.260803                   tensor(0.3219)  \n",
      "0   25457.645615  tensor(0.3960, device='cuda:0')  \n",
      "1   25458.793839                   tensor(0.3960)  \n",
      "2   25457.916458                   tensor(0.3960)  \n",
      "3   25458.601196                   tensor(0.3960)  \n",
      "0   48458.278534  tensor(0.4595, device='cuda:0')  \n",
      "1   48459.773895                   tensor(0.4595)  \n",
      "2   48458.675262                   tensor(0.4595)  \n",
      "3   48460.475800                   tensor(0.4595)  \n",
      "0   95341.491638  tensor(0.4694, device='cuda:0')  \n",
      "1   97191.574036                   tensor(0.4694)  \n",
      "2   95342.338501                   tensor(0.4694)  \n",
      "3   95346.099792                   tensor(0.4694)  \n",
      "0  191488.169678  tensor(0.3393, device='cuda:0')  \n",
      "1  256955.576904                   tensor(0.3468)  \n",
      "2  193867.533691                   tensor(0.3624)  \n",
      "3  201126.933105                   tensor(0.3564)  \n",
      "0  390854.397705  tensor(0.6498, device='cuda:0')  \n",
      "1  516676.205566                   tensor(0.4615)  \n",
      "2  394445.401123                   tensor(0.6498)  \n",
      "3  390876.828125                   tensor(0.6498)  \n",
      "\n",
      "[28 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Store results in a list of DataFrames\n",
    "experiment_results = []\n",
    "\n",
    "# Experiment 1: Well-separated clusters with increasing dimensions\n",
    "for dim in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    X_tensor = generate_data(1000, dim, centers_4d, random_state).to(device)\n",
    "    \n",
    "    # Fix the label generation to match the data size\n",
    "    n_per_cluster = 1000 // 4  # 1000 samples divided by 4 clusters\n",
    "    y_tensor = torch.cat([torch.ones(n_per_cluster) * i for i in range(4)]).long()\n",
    "    \n",
    "    df = run_experiment(X_tensor, y_tensor, dim, 1000, 4, \"Experiment 1\")\n",
    "    experiment_results.append(df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results = pd.concat(experiment_results)\n",
    "\n",
    "file_name = 'gmm_adam_vs_em_varying_dimensions.xlsx'\n",
    "folder = 'results'\n",
    "final_results.to_excel(f'{folder}/{file_name}', index=False)\n",
    "\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 2: Increasing Number of Clusters (Fixed Dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Experiment 2 with 4 dimensions, with 2 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.28389835357666, Time taken: 0.0120 seconds\n",
      "Adam (lr=0.1) converged in 19 iterations with log-likelihood: -6.286192893981934, Time taken: 0.1008 seconds\n",
      "Adam (lr=0.01) converged in 36 iterations with log-likelihood: -6.2839460372924805, Time taken: 0.1796 seconds\n",
      "Adam (lr=0.001) converged in 118 iterations with log-likelihood: -6.284086227416992, Time taken: 0.6731 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.963937282562256, Time taken: 0.0118 seconds\n",
      "Adam (lr=0.1) converged in 44 iterations with log-likelihood: -6.964127063751221, Time taken: 0.4809 seconds\n",
      "Adam (lr=0.01) converged in 14 iterations with log-likelihood: -6.964780807495117, Time taken: 0.1788 seconds\n",
      "Adam (lr=0.001) converged in 152 iterations with log-likelihood: -6.964132308959961, Time taken: 1.7366 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 8 clusters and 1000 samples\n",
      "EM converged in 12 iterations with log-likelihood: -7.894438743591309, Time taken: 0.0825 seconds\n",
      "Adam (lr=0.1) converged in 1000 iterations with log-likelihood: -8.006054878234863, Time taken: 16.4236 seconds\n",
      "Adam (lr=0.01) converged in 1000 iterations with log-likelihood: -8.27218246459961, Time taken: 16.8499 seconds\n",
      "Adam (lr=0.001) converged in 1000 iterations with log-likelihood: -9.260305404663086, Time taken: 16.9345 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 16 clusters and 1000 samples\n",
      "EM converged in 17 iterations with log-likelihood: -8.320568084716797, Time taken: 0.1253 seconds\n",
      "Adam (lr=0.1) converged in 200 iterations with log-likelihood: -8.367509841918945, Time taken: 7.5348 seconds\n",
      "Adam (lr=0.01) converged in 1000 iterations with log-likelihood: -8.372735977172852, Time taken: 41.6838 seconds\n",
      "Adam (lr=0.001) converged in 1000 iterations with log-likelihood: -8.496332168579102, Time taken: 41.6588 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 32 clusters and 1000 samples\n",
      "EM converged in 13 iterations with log-likelihood: -8.876809120178223, Time taken: 0.1898 seconds\n",
      "Adam (lr=0.1) converged in 264 iterations with log-likelihood: -8.949933052062988, Time taken: 22.5889 seconds\n",
      "Adam (lr=0.01) converged in 1000 iterations with log-likelihood: -8.950286865234375, Time taken: 93.8291 seconds\n",
      "Adam (lr=0.001) converged in 1000 iterations with log-likelihood: -9.02227783203125, Time taken: 76.5582 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 64 clusters and 1000 samples\n",
      "EM converged in 11 iterations with log-likelihood: -9.31995677947998, Time taken: 0.2306 seconds\n",
      "Adam (lr=0.1) converged in 154 iterations with log-likelihood: -9.5944242477417, Time taken: 20.7254 seconds\n",
      "Adam (lr=0.01) converged in 1000 iterations with log-likelihood: -9.544824600219727, Time taken: 152.5827 seconds\n",
      "Adam (lr=0.001) converged in 1000 iterations with log-likelihood: -9.93581485748291, Time taken: 142.0918 seconds\n",
      "\n",
      " Running Experiment 2 with 4 dimensions, with 128 clusters and 1000 samples\n",
      "EM converged in 13 iterations with log-likelihood: -8.579142570495605, Time taken: 0.5323 seconds\n",
      "Adam (lr=0.1) converged in 364 iterations with log-likelihood: -9.89617919921875, Time taken: 112.6842 seconds\n",
      "Adam (lr=0.01) converged in 265 iterations with log-likelihood: -9.378209114074707, Time taken: 81.1363 seconds\n",
      "Adam (lr=0.001) converged in 684 iterations with log-likelihood: -9.431344985961914, Time taken: 162.1186 seconds\n",
      "     experiment method  n_features  n_clusters  data_size  learning_rate  \\\n",
      "0  Experiment 2     EM           4           2       1000            NaN   \n",
      "1  Experiment 2   Adam           4           2       1000          0.100   \n",
      "2  Experiment 2   Adam           4           2       1000          0.010   \n",
      "3  Experiment 2   Adam           4           2       1000          0.001   \n",
      "0  Experiment 2     EM           4           4       1000            NaN   \n",
      "1  Experiment 2   Adam           4           4       1000          0.100   \n",
      "2  Experiment 2   Adam           4           4       1000          0.010   \n",
      "3  Experiment 2   Adam           4           4       1000          0.001   \n",
      "0  Experiment 2     EM           4           8       1000            NaN   \n",
      "1  Experiment 2   Adam           4           8       1000          0.100   \n",
      "2  Experiment 2   Adam           4           8       1000          0.010   \n",
      "3  Experiment 2   Adam           4           8       1000          0.001   \n",
      "0  Experiment 2     EM           4          16       1000            NaN   \n",
      "1  Experiment 2   Adam           4          16       1000          0.100   \n",
      "2  Experiment 2   Adam           4          16       1000          0.010   \n",
      "3  Experiment 2   Adam           4          16       1000          0.001   \n",
      "0  Experiment 2     EM           4          32       1000            NaN   \n",
      "1  Experiment 2   Adam           4          32       1000          0.100   \n",
      "2  Experiment 2   Adam           4          32       1000          0.010   \n",
      "3  Experiment 2   Adam           4          32       1000          0.001   \n",
      "0  Experiment 2     EM           4          64       1000            NaN   \n",
      "1  Experiment 2   Adam           4          64       1000          0.100   \n",
      "2  Experiment 2   Adam           4          64       1000          0.010   \n",
      "3  Experiment 2   Adam           4          64       1000          0.001   \n",
      "0  Experiment 2     EM           4         128       1000            NaN   \n",
      "1  Experiment 2   Adam           4         128       1000          0.100   \n",
      "2  Experiment 2   Adam           4         128       1000          0.010   \n",
      "3  Experiment 2   Adam           4         128       1000          0.001   \n",
      "\n",
      "   iterations  log_likelihood  time_taken  rand_score  ...  v_measure_score  \\\n",
      "0           2       -6.283898    0.011998    1.000000  ...         1.000000   \n",
      "1          19       -6.286193    0.100802    1.000000  ...         1.000000   \n",
      "2          36       -6.283946    0.179562    1.000000  ...         1.000000   \n",
      "3         118       -6.284086    0.673135    1.000000  ...         1.000000   \n",
      "0           2       -6.963937    0.011787    1.000000  ...         1.000000   \n",
      "1          44       -6.964127    0.480881    1.000000  ...         1.000000   \n",
      "2          14       -6.964781    0.178758    1.000000  ...         1.000000   \n",
      "3         152       -6.964132    1.736588    1.000000  ...         1.000000   \n",
      "0          12       -7.894439    0.082518    0.960983  ...         0.935280   \n",
      "1        1000       -8.006055   16.423619    0.960899  ...         0.935141   \n",
      "2        1000       -8.272182   16.849870    0.960911  ...         0.935161   \n",
      "3        1000       -9.260305   16.934480    0.960911  ...         0.935161   \n",
      "0          17       -8.320568    0.125341    0.990324  ...         0.976523   \n",
      "1         200       -8.367510    7.534806    0.990257  ...         0.976337   \n",
      "2        1000       -8.372736   41.683758    0.990225  ...         0.976248   \n",
      "3        1000       -8.496332   41.658810    0.990275  ...         0.976388   \n",
      "0          13       -8.876809    0.189823    0.995394  ...         0.982441   \n",
      "1         264       -8.949933   22.588908    0.995113  ...         0.981054   \n",
      "2        1000       -8.950287   93.829072    0.995117  ...         0.981072   \n",
      "3        1000       -9.022278   76.558154    0.995231  ...         0.981599   \n",
      "0          11       -9.319957    0.230562    0.995922  ...         0.975785   \n",
      "1         154       -9.594424   20.725363    0.995970  ...         0.976173   \n",
      "2        1000       -9.544825  152.582729    0.995853  ...         0.975253   \n",
      "3        1000       -9.935815  142.091783    0.995870  ...         0.975382   \n",
      "0          13       -8.579143    0.532287    0.997593  ...         0.971934   \n",
      "1         364       -9.896179  112.684160    0.997553  ...              NaN   \n",
      "2         265       -9.378209   81.136302    0.997653  ...         0.972993   \n",
      "3         684       -9.431345  162.118599    0.997638  ...         0.972399   \n",
      "\n",
      "   purity_score                              classification_report  \\\n",
      "0      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "1      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "2      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "3      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "0      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "1      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "2      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "3      1.000000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "0      0.875000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1      0.875000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2      0.875000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3      0.875000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3      0.937500  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0      0.906250  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1      0.906250  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2      0.906250  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3      0.906250  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0      0.876116  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1      0.869420  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2      0.878348  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3      0.878348  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "\n",
      "                                    confusion_matrix  silhouette_score  \\\n",
      "0  [[tensor(500, dtype=torch.int32), tensor(0, dt...          0.885985   \n",
      "1  [[tensor(500, dtype=torch.int32), tensor(0, dt...          0.885985   \n",
      "2  [[tensor(500, dtype=torch.int32), tensor(0, dt...          0.885985   \n",
      "3  [[tensor(500, dtype=torch.int32), tensor(0, dt...          0.885985   \n",
      "0  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.883889   \n",
      "1  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.883888   \n",
      "2  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.883888   \n",
      "3  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.883888   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.715445   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.709448   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.715576   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.715576   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.717321   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.719329   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.719227   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.719265   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.713907   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.705452   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.707851   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.708054   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.629719   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.622789   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.630512   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.629893   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...               NaN   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...               NaN   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...               NaN   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...               NaN   \n",
      "\n",
      "   davies_bouldin_index  calinski_harabasz_score     bic_score     aic_score  \\\n",
      "0              0.162071             33905.152344  12768.122070  12625.796707   \n",
      "1              0.162071             33905.156250  12772.710938  12630.385788   \n",
      "2              0.162071             33905.156250  12768.216797  12625.892075   \n",
      "3              0.162071             33905.156250  12768.498047  12626.172455   \n",
      "0              0.165108             41706.386719  14335.432617  14045.874565   \n",
      "1              0.165108             41706.375000  14335.811523  14046.254128   \n",
      "2              0.165108             41706.375000  14337.119141  14047.561615   \n",
      "3              0.165108             41706.375000  14335.822266  14046.264618   \n",
      "0              0.659196              5153.500977  16610.900391  16026.877487   \n",
      "1              0.794517              5136.521484  16834.132812  16250.109756   \n",
      "2              0.660372              5153.607422  17366.388672  16782.364929   \n",
      "3              0.660372              5153.607422  19342.634766  18758.610809   \n",
      "0              0.610893              5044.585938  18157.041016  16986.007080   \n",
      "1              0.567572              5053.629883  18250.171875  17079.139526   \n",
      "2              0.576174              5052.587402  18260.541016  17089.508179   \n",
      "3              0.565066              5053.880371  18505.755859  17334.723022   \n",
      "0              0.490513              2999.676758  20916.556641  18569.589294   \n",
      "1              0.561957              2989.475830  21061.634766  18714.667175   \n",
      "2              0.532772              2995.980225  21062.335938  18715.369141   \n",
      "3              0.537247              2993.673096  21205.166016  18858.199219   \n",
      "0              0.626691               892.256775  24479.705078  19812.317017   \n",
      "1              0.700005               887.613892  25006.683594  20339.294556   \n",
      "2              0.626528               892.776611  24911.451172  20244.063232   \n",
      "3              0.628854               892.194397  25662.152344  20994.764526   \n",
      "0              0.658288               509.907501  28419.070312  19211.823486   \n",
      "1                   NaN                      NaN  30779.199219  21571.953125   \n",
      "2              0.649213               510.089050  29850.996094  20643.750732   \n",
      "3              0.661807               509.843933  29946.218750  20738.970215   \n",
      "\n",
      "                        dunn_index  \n",
      "0  tensor(2.3235, device='cuda:0')  \n",
      "1                   tensor(2.3235)  \n",
      "2                   tensor(2.3235)  \n",
      "3                   tensor(2.3235)  \n",
      "0  tensor(1.9916, device='cuda:0')  \n",
      "1                   tensor(1.9916)  \n",
      "2                   tensor(1.9916)  \n",
      "3                   tensor(1.9916)  \n",
      "0  tensor(0.0210, device='cuda:0')  \n",
      "1                   tensor(0.0414)  \n",
      "2                   tensor(0.0471)  \n",
      "3                   tensor(0.0471)  \n",
      "0  tensor(0.0667, device='cuda:0')  \n",
      "1                   tensor(0.0300)  \n",
      "2                   tensor(0.0687)  \n",
      "3                   tensor(0.0687)  \n",
      "0  tensor(0.0881, device='cuda:0')  \n",
      "1                   tensor(0.0808)  \n",
      "2                   tensor(0.0731)  \n",
      "3                   tensor(0.0854)  \n",
      "0  tensor(0.0564, device='cuda:0')  \n",
      "1                   tensor(0.0715)  \n",
      "2                   tensor(0.0710)  \n",
      "3                   tensor(0.0710)  \n",
      "0  tensor(0.0715, device='cuda:0')  \n",
      "1                   tensor(0.0622)  \n",
      "2                   tensor(0.0928)  \n",
      "3                   tensor(0.0490)  \n",
      "\n",
      "[28 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Store results in a list of DataFrames\n",
    "experiment_results = []\n",
    "\n",
    "# Function to generate random cluster centers\n",
    "def generate_random_centers(n_clusters, n_features, scale=10):\n",
    "    \"\"\"Generates random centers for the clusters.\"\"\"\n",
    "    return [np.random.randn(n_features) * scale for _ in range(n_clusters)]\n",
    "\n",
    "# Experiment 2: Increasing number of clusters with fixed dimensions\n",
    "for n_clusters in [2, 4, 8, 16, 32, 64, 128]:\n",
    "    centers = generate_random_centers(n_clusters, 4)  # Random cluster centers for each experiment\n",
    "    X_tensor = generate_data(1000, 4, centers, random_state).to(device)\n",
    "    y_tensor = torch.cat([torch.ones(1000 // n_clusters) * i for i in range(n_clusters)]).long()\n",
    "    df = run_experiment(X_tensor, y_tensor, 4, 1000, n_clusters, \"Experiment 2\")\n",
    "    experiment_results.append(df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results = pd.concat(experiment_results)\n",
    "\n",
    "file_name = 'gmm_adam_vs_em_varying_clusters.xlsx'\n",
    "folder = 'results'\n",
    "final_results.to_excel(f'{folder}/{file_name}', index=False)\n",
    "\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 3: Fixed Number of Clusters and Dimensions, Increasing Data Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Experiment 3 with 4 dimensions, with 4 clusters and 100 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.779717922210693, Time taken: 0.0167 seconds\n",
      "Adam (lr=0.1) converged in 46 iterations with log-likelihood: -6.7802653312683105, Time taken: 0.5551 seconds\n",
      "Adam (lr=0.01) converged in 82 iterations with log-likelihood: -6.779794692993164, Time taken: 0.9292 seconds\n",
      "Adam (lr=0.001) converged in 526 iterations with log-likelihood: -6.780829906463623, Time taken: 3.3411 seconds\n",
      "\n",
      " Running Experiment 3 with 4 dimensions, with 4 clusters and 500 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.931983470916748, Time taken: 0.0060 seconds\n",
      "Adam (lr=0.1) converged in 44 iterations with log-likelihood: -6.932250499725342, Time taken: 0.2743 seconds\n",
      "Adam (lr=0.01) converged in 51 iterations with log-likelihood: -6.932024002075195, Time taken: 0.3136 seconds\n",
      "Adam (lr=0.001) converged in 210 iterations with log-likelihood: -6.9322710037231445, Time taken: 1.2294 seconds\n",
      "\n",
      " Running Experiment 3 with 4 dimensions, with 4 clusters and 1000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -6.963935375213623, Time taken: 0.0054 seconds\n",
      "Adam (lr=0.1) converged in 28 iterations with log-likelihood: -6.96526575088501, Time taken: 0.1384 seconds\n",
      "Adam (lr=0.01) converged in 14 iterations with log-likelihood: -6.964781761169434, Time taken: 0.0892 seconds\n",
      "Adam (lr=0.001) converged in 152 iterations with log-likelihood: -6.964130401611328, Time taken: 1.2727 seconds\n",
      "\n",
      " Running Experiment 3 with 4 dimensions, with 4 clusters and 5000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -7.0189528465271, Time taken: 0.0318 seconds\n",
      "Adam (lr=0.1) converged in 37 iterations with log-likelihood: -7.019401550292969, Time taken: 0.3741 seconds\n",
      "Adam (lr=0.01) converged in 13 iterations with log-likelihood: -7.01919412612915, Time taken: 0.1496 seconds\n",
      "Adam (lr=0.001) converged in 56 iterations with log-likelihood: -7.019062519073486, Time taken: 0.5808 seconds\n",
      "\n",
      " Running Experiment 3 with 4 dimensions, with 4 clusters and 10000 samples\n",
      "EM converged in 2 iterations with log-likelihood: -7.046177864074707, Time taken: 0.0479 seconds\n",
      "Adam (lr=0.1) converged in 47 iterations with log-likelihood: -7.046360015869141, Time taken: 0.5300 seconds\n",
      "Adam (lr=0.01) converged in 8 iterations with log-likelihood: -7.0464582443237305, Time taken: 0.1074 seconds\n",
      "Adam (lr=0.001) converged in 37 iterations with log-likelihood: -7.046256065368652, Time taken: 0.3077 seconds\n",
      "     experiment method  n_features  n_clusters  data_size  learning_rate  \\\n",
      "0  Experiment 3     EM           4           4        100            NaN   \n",
      "1  Experiment 3   Adam           4           4        100          0.100   \n",
      "2  Experiment 3   Adam           4           4        100          0.010   \n",
      "3  Experiment 3   Adam           4           4        100          0.001   \n",
      "0  Experiment 3     EM           4           4        500            NaN   \n",
      "1  Experiment 3   Adam           4           4        500          0.100   \n",
      "2  Experiment 3   Adam           4           4        500          0.010   \n",
      "3  Experiment 3   Adam           4           4        500          0.001   \n",
      "0  Experiment 3     EM           4           4       1000            NaN   \n",
      "1  Experiment 3   Adam           4           4       1000          0.100   \n",
      "2  Experiment 3   Adam           4           4       1000          0.010   \n",
      "3  Experiment 3   Adam           4           4       1000          0.001   \n",
      "0  Experiment 3     EM           4           4       5000            NaN   \n",
      "1  Experiment 3   Adam           4           4       5000          0.100   \n",
      "2  Experiment 3   Adam           4           4       5000          0.010   \n",
      "3  Experiment 3   Adam           4           4       5000          0.001   \n",
      "0  Experiment 3     EM           4           4      10000            NaN   \n",
      "1  Experiment 3   Adam           4           4      10000          0.100   \n",
      "2  Experiment 3   Adam           4           4      10000          0.010   \n",
      "3  Experiment 3   Adam           4           4      10000          0.001   \n",
      "\n",
      "   iterations  log_likelihood  time_taken  rand_score  ...  v_measure_score  \\\n",
      "0           2       -6.779718    0.016738      1.0000  ...         1.000000   \n",
      "1          46       -6.780265    0.555101      1.0000  ...         1.000000   \n",
      "2          82       -6.779795    0.929201      1.0000  ...         1.000000   \n",
      "3         526       -6.780830    3.341129      1.0000  ...         1.000000   \n",
      "0           2       -6.931983    0.006011      1.0000  ...         1.000000   \n",
      "1          44       -6.932250    0.274349      1.0000  ...         1.000000   \n",
      "2          51       -6.932024    0.313567      1.0000  ...         1.000000   \n",
      "3         210       -6.932271    1.229442      1.0000  ...         1.000000   \n",
      "0           2       -6.963935    0.005388      1.0000  ...         1.000000   \n",
      "1          28       -6.965266    0.138357      1.0000  ...         1.000000   \n",
      "2          14       -6.964782    0.089183      1.0000  ...         1.000000   \n",
      "3         152       -6.964130    1.272744      1.0000  ...         1.000000   \n",
      "0           2       -7.018953    0.031815      0.9998  ...         0.998827   \n",
      "1          37       -7.019402    0.374089      0.9998  ...         0.998827   \n",
      "2          13       -7.019194    0.149641      0.9998  ...         0.998827   \n",
      "3          56       -7.019063    0.580781      0.9998  ...         0.998827   \n",
      "0           2       -7.046178    0.047947      0.9999  ...         0.999363   \n",
      "1          47       -7.046360    0.529995      0.9999  ...         0.999363   \n",
      "2           8       -7.046458    0.107372      0.9999  ...         0.999363   \n",
      "3          37       -7.046256    0.307670      0.9999  ...         0.999363   \n",
      "\n",
      "   purity_score                              classification_report  \\\n",
      "0        1.0000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1        1.0000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2        1.0000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3        1.0000  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "1        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "2        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "3        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "0        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "1        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "2        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "3        1.0000  {0: {'precision': 1.0, 'recall': 1.0, 'f1-scor...   \n",
      "0        0.9998  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1        0.9998  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2        0.9998  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3        0.9998  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "0        0.9999  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "1        0.9999  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "2        0.9999  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "3        0.9999  {0: {'precision': 0.0, 'recall': 0.0, 'f1-scor...   \n",
      "\n",
      "                                    confusion_matrix  silhouette_score  \\\n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.661128   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.661126   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.661126   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.661126   \n",
      "0  [[tensor(125, dtype=torch.int32), tensor(0, dt...          0.674596   \n",
      "1  [[tensor(125, dtype=torch.int32), tensor(0, dt...          0.674596   \n",
      "2  [[tensor(125, dtype=torch.int32), tensor(0, dt...          0.674596   \n",
      "3  [[tensor(125, dtype=torch.int32), tensor(0, dt...          0.674596   \n",
      "0  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "1  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "2  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "3  [[tensor(250, dtype=torch.int32), tensor(0, dt...          0.672938   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.672149   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.672149   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.672149   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(0, dtyp...          0.672149   \n",
      "0  [[tensor(0, dtype=torch.int32), tensor(2499, d...          0.670459   \n",
      "1  [[tensor(0, dtype=torch.int32), tensor(2499, d...          0.670459   \n",
      "2  [[tensor(0, dtype=torch.int32), tensor(2499, d...          0.670459   \n",
      "3  [[tensor(0, dtype=torch.int32), tensor(2499, d...          0.670459   \n",
      "\n",
      "   davies_bouldin_index  calinski_harabasz_score      bic_score  \\\n",
      "0              0.475871               366.252472    1627.648682   \n",
      "1              0.475871               366.252502    1627.758179   \n",
      "2              0.475871               366.252502    1627.664062   \n",
      "3              0.475871               366.252502    1627.871094   \n",
      "0              0.459629              1945.829102    7298.645508   \n",
      "1              0.459629              1945.828735    7298.912598   \n",
      "2              0.459629              1945.828735    7298.686035   \n",
      "3              0.459629              1945.828735    7298.933105   \n",
      "0              0.463784              3787.711426   14335.428711   \n",
      "1              0.463784              3787.711182   14338.088867   \n",
      "2              0.463784              3787.711182   14337.121094   \n",
      "3              0.463784              3787.711182   14335.818359   \n",
      "0              0.464491             18805.712891   70692.046875   \n",
      "1              0.464491             18805.710938   70696.531250   \n",
      "2              0.464491             18805.710938   70694.453125   \n",
      "3              0.464491             18805.710938   70693.140625   \n",
      "0              0.465904             37046.597656  141466.968750   \n",
      "1              0.465904             37046.597656  141470.609375   \n",
      "2              0.465904             37046.597656  141472.578125   \n",
      "3              0.465904             37046.597656  141468.531250   \n",
      "\n",
      "       aic_score                       dunn_index  \n",
      "0    1473.943584  tensor(0.6595, device='cuda:0')  \n",
      "1    1474.053066                   tensor(0.6595)  \n",
      "2    1473.958939                   tensor(0.6595)  \n",
      "3    1474.165981                   tensor(0.6595)  \n",
      "0    7049.983471  tensor(0.4269, device='cuda:0')  \n",
      "1    7050.250500                   tensor(0.4269)  \n",
      "2    7050.024002                   tensor(0.4269)  \n",
      "3    7050.271004                   tensor(0.4269)  \n",
      "0   14045.870750  tensor(0.3219, device='cuda:0')  \n",
      "1   14048.531502                   tensor(0.3219)  \n",
      "2   14047.563522                   tensor(0.3219)  \n",
      "3   14046.260803                   tensor(0.3219)  \n",
      "0   70307.528465  tensor(0.1771, device='cuda:0')  \n",
      "1   70312.015503                   tensor(0.1771)  \n",
      "2   70309.941261                   tensor(0.1771)  \n",
      "3   70308.625191                   tensor(0.1771)  \n",
      "0  141041.557281  tensor(0.1575, device='cuda:0')  \n",
      "1  141045.200317                   tensor(0.1575)  \n",
      "2  141047.164886                   tensor(0.1575)  \n",
      "3  141043.121307                   tensor(0.1575)  \n",
      "\n",
      "[20 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "# Store results in a list of DataFrames\n",
    "experiment_results = []\n",
    "\n",
    "# Experiment 3: Fixed clusters and dimensions, increasing data sizes\n",
    "for size in [100, 500, 1000, 5000, 10000]:\n",
    "    X_tensor = generate_data(size, 4, centers_4d, random_state).to(device)\n",
    "    y_tensor = torch.cat([torch.zeros(size//4), torch.ones(size//4), 2 * torch.ones(size//4), 3 * torch.ones(size//4)]).long()\n",
    "    df = run_experiment(X_tensor, y_tensor, 4, size, 4, \"Experiment 3\")\n",
    "    experiment_results.append(df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results = pd.concat(experiment_results)\n",
    "\n",
    "file_name = 'gmm_adam_vs_em_varying_datasizes.xlsx'\n",
    "folder = 'results'\n",
    "final_results.to_excel(f'{folder}/{file_name}', index=False)\n",
    "\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Fixed Number of Clusters, Dimensions, Data Sizes - Different initial points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 0\n",
      "EM converged in 17 iterations with log-likelihood: -7.31906795501709, Time taken: 0.0377 seconds\n",
      "Adam (lr=10) converged in 386 iterations with log-likelihood: -19.558486938476562, Time taken: 3.1530 seconds\n",
      "Adam (lr=1.0) converged in 104 iterations with log-likelihood: -8.479379653930664, Time taken: 1.2602 seconds\n",
      "Adam (lr=0.1) converged in 51 iterations with log-likelihood: -8.124852180480957, Time taken: 0.6178 seconds\n",
      "Adam (lr=0.01) converged in 1148 iterations with log-likelihood: -7.318942070007324, Time taken: 13.2956 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 1\n",
      "EM converged in 13 iterations with log-likelihood: -7.033401012420654, Time taken: 0.0253 seconds\n",
      "Adam (lr=10) converged in 30 iterations with log-likelihood: -18.858457565307617, Time taken: 0.3543 seconds\n",
      "Adam (lr=1.0) converged in 2 iterations with log-likelihood: -9.503402709960938, Time taken: 0.0820 seconds\n",
      "Adam (lr=0.1) converged in 311 iterations with log-likelihood: -7.04107141494751, Time taken: 3.1569 seconds\n",
      "Adam (lr=0.01) converged in 983 iterations with log-likelihood: -7.503243446350098, Time taken: 11.7565 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 2\n",
      "EM converged in 4 iterations with log-likelihood: -7.401198387145996, Time taken: 0.0120 seconds\n",
      "Adam (lr=10) converged in 328 iterations with log-likelihood: -17.982196807861328, Time taken: 3.9561 seconds\n",
      "Adam (lr=1.0) converged in 11 iterations with log-likelihood: -9.974396705627441, Time taken: 0.1516 seconds\n",
      "Adam (lr=0.1) converged in 76 iterations with log-likelihood: -7.785860538482666, Time taken: 0.9370 seconds\n",
      "Adam (lr=0.01) converged in 509 iterations with log-likelihood: -7.78552770614624, Time taken: 5.9597 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 3\n",
      "EM converged in 11 iterations with log-likelihood: -7.385015964508057, Time taken: 0.0220 seconds\n",
      "Adam (lr=10) converged in 15 iterations with log-likelihood: -18.659469604492188, Time taken: 0.2044 seconds\n",
      "Adam (lr=1.0) converged in 15 iterations with log-likelihood: -10.533008575439453, Time taken: 0.1882 seconds\n",
      "Adam (lr=0.1) converged in 242 iterations with log-likelihood: -7.041301727294922, Time taken: 2.7928 seconds\n",
      "Adam (lr=0.01) converged in 794 iterations with log-likelihood: -7.31130838394165, Time taken: 9.2636 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 4\n",
      "EM converged in 13 iterations with log-likelihood: -7.306077003479004, Time taken: 0.0258 seconds\n",
      "Adam (lr=10) converged in 76 iterations with log-likelihood: -15.889202117919922, Time taken: 0.9105 seconds\n",
      "Adam (lr=1.0) converged in 54 iterations with log-likelihood: -8.793587684631348, Time taken: 0.6810 seconds\n",
      "Adam (lr=0.1) converged in 235 iterations with log-likelihood: -7.190877437591553, Time taken: 2.7778 seconds\n",
      "Adam (lr=0.01) converged in 782 iterations with log-likelihood: -7.722507953643799, Time taken: 9.0671 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 5\n",
      "EM converged in 5 iterations with log-likelihood: -7.0726494789123535, Time taken: 0.0136 seconds\n",
      "Adam (lr=10) converged in 20 iterations with log-likelihood: -18.684919357299805, Time taken: 0.2493 seconds\n",
      "Adam (lr=1.0) converged in 76 iterations with log-likelihood: -8.529754638671875, Time taken: 0.9091 seconds\n",
      "Adam (lr=0.1) converged in 140 iterations with log-likelihood: -7.075125217437744, Time taken: 0.8530 seconds\n",
      "Adam (lr=0.01) converged in 700 iterations with log-likelihood: -7.173701286315918, Time taken: 7.4521 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 6\n",
      "EM converged in 7 iterations with log-likelihood: -7.265392780303955, Time taken: 0.0166 seconds\n",
      "Adam (lr=10) converged in 20 iterations with log-likelihood: -19.6925106048584, Time taken: 0.2462 seconds\n",
      "Adam (lr=1.0) converged in 95 iterations with log-likelihood: -8.562828063964844, Time taken: 1.1430 seconds\n",
      "Adam (lr=0.1) converged in 96 iterations with log-likelihood: -7.524155616760254, Time taken: 1.1591 seconds\n",
      "Adam (lr=0.01) converged in 824 iterations with log-likelihood: -7.45194673538208, Time taken: 10.5931 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 7\n",
      "EM converged in 8 iterations with log-likelihood: -7.368079662322998, Time taken: 0.0163 seconds\n",
      "Adam (lr=10) converged in 29 iterations with log-likelihood: -19.78238296508789, Time taken: 0.3192 seconds\n",
      "Adam (lr=1.0) converged in 177 iterations with log-likelihood: -7.836955547332764, Time taken: 1.7943 seconds\n",
      "Adam (lr=0.1) converged in 180 iterations with log-likelihood: -7.04953145980835, Time taken: 1.8074 seconds\n",
      "Adam (lr=0.01) converged in 805 iterations with log-likelihood: -7.099215984344482, Time taken: 9.2821 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 8\n",
      "EM converged in 8 iterations with log-likelihood: -7.030754089355469, Time taken: 0.0157 seconds\n",
      "Adam (lr=10) converged in 13 iterations with log-likelihood: -17.73198699951172, Time taken: 0.1348 seconds\n",
      "Adam (lr=1.0) converged in 36 iterations with log-likelihood: -8.77760124206543, Time taken: 0.3534 seconds\n",
      "Adam (lr=0.1) converged in 117 iterations with log-likelihood: -7.034559726715088, Time taken: 1.1881 seconds\n",
      "Adam (lr=0.01) converged in 587 iterations with log-likelihood: -7.1009674072265625, Time taken: 5.8265 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: random with random state: 9\n",
      "EM converged in 5 iterations with log-likelihood: -7.423723220825195, Time taken: 0.0127 seconds\n",
      "Adam (lr=10) converged in 34 iterations with log-likelihood: -19.975109100341797, Time taken: 0.3821 seconds\n",
      "Adam (lr=1.0) converged in 18 iterations with log-likelihood: -10.899805068969727, Time taken: 0.2708 seconds\n",
      "Adam (lr=0.1) converged in 207 iterations with log-likelihood: -7.209980010986328, Time taken: 2.3874 seconds\n",
      "Adam (lr=0.01) converged in 409 iterations with log-likelihood: -7.94024658203125, Time taken: 4.7224 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 0\n",
      "EM converged in 10 iterations with log-likelihood: -6.963935852050781, Time taken: 0.0174 seconds\n",
      "Adam (lr=10) converged in 278 iterations with log-likelihood: -16.484102249145508, Time taken: 2.6783 seconds\n",
      "Adam (lr=1.0) converged in 5 iterations with log-likelihood: -9.432279586791992, Time taken: 0.0590 seconds\n",
      "Adam (lr=0.1) converged in 75 iterations with log-likelihood: -7.822702407836914, Time taken: 0.7450 seconds\n",
      "Adam (lr=0.01) converged in 481 iterations with log-likelihood: -7.934817790985107, Time taken: 4.7652 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 1\n",
      "EM converged in 8 iterations with log-likelihood: -7.033401012420654, Time taken: 0.0149 seconds\n",
      "Adam (lr=10) converged in 113 iterations with log-likelihood: -16.627910614013672, Time taken: 1.1864 seconds\n",
      "Adam (lr=1.0) converged in 53 iterations with log-likelihood: -8.04840087890625, Time taken: 0.5600 seconds\n",
      "Adam (lr=0.1) converged in 123 iterations with log-likelihood: -7.446845054626465, Time taken: 1.2553 seconds\n",
      "Adam (lr=0.01) converged in 496 iterations with log-likelihood: -7.422713279724121, Time taken: 5.6624 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 2\n",
      "EM converged in 11 iterations with log-likelihood: -7.07567024230957, Time taken: 0.0214 seconds\n",
      "Adam (lr=10) converged in 163 iterations with log-likelihood: -14.249600410461426, Time taken: 1.7504 seconds\n",
      "Adam (lr=1.0) converged in 38 iterations with log-likelihood: -8.513176918029785, Time taken: 0.4523 seconds\n",
      "Adam (lr=0.1) converged in 164 iterations with log-likelihood: -7.675034523010254, Time taken: 2.1648 seconds\n",
      "Adam (lr=0.01) converged in 695 iterations with log-likelihood: -7.898029327392578, Time taken: 9.6184 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 3\n",
      "EM converged in 7 iterations with log-likelihood: -7.575973987579346, Time taken: 0.0145 seconds\n",
      "Adam (lr=10) converged in 124 iterations with log-likelihood: -18.599641799926758, Time taken: 1.3378 seconds\n",
      "Adam (lr=1.0) converged in 39 iterations with log-likelihood: -8.764984130859375, Time taken: 0.4075 seconds\n",
      "Adam (lr=0.1) converged in 89 iterations with log-likelihood: -8.028463363647461, Time taken: 0.9365 seconds\n",
      "Adam (lr=0.01) converged in 532 iterations with log-likelihood: -7.903397083282471, Time taken: 5.7364 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 4\n",
      "EM converged in 10 iterations with log-likelihood: -7.348397254943848, Time taken: 0.0183 seconds\n",
      "Adam (lr=10) converged in 58 iterations with log-likelihood: -17.324874877929688, Time taken: 0.5574 seconds\n",
      "Adam (lr=1.0) converged in 61 iterations with log-likelihood: -8.387242317199707, Time taken: 0.6206 seconds\n",
      "Adam (lr=0.1) converged in 105 iterations with log-likelihood: -7.128871440887451, Time taken: 1.0301 seconds\n",
      "Adam (lr=0.01) converged in 687 iterations with log-likelihood: -6.971232891082764, Time taken: 6.9889 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 5\n",
      "EM converged in 7 iterations with log-likelihood: -7.072649002075195, Time taken: 0.0064 seconds\n",
      "Adam (lr=10) converged in 168 iterations with log-likelihood: -17.617074966430664, Time taken: 1.0016 seconds\n",
      "Adam (lr=1.0) converged in 26 iterations with log-likelihood: -9.003209114074707, Time taken: 0.2566 seconds\n",
      "Adam (lr=0.1) converged in 239 iterations with log-likelihood: -7.077488422393799, Time taken: 2.4583 seconds\n",
      "Adam (lr=0.01) converged in 358 iterations with log-likelihood: -7.433394908905029, Time taken: 3.5426 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 6\n",
      "EM converged in 20 iterations with log-likelihood: -7.259280681610107, Time taken: 0.0409 seconds\n",
      "Adam (lr=10) converged in 445 iterations with log-likelihood: -18.1282901763916, Time taken: 6.7439 seconds\n",
      "Adam (lr=1.0) converged in 65 iterations with log-likelihood: -8.38845157623291, Time taken: 0.9761 seconds\n",
      "Adam (lr=0.1) converged in 273 iterations with log-likelihood: -7.122843265533447, Time taken: 4.5275 seconds\n",
      "Adam (lr=0.01) converged in 565 iterations with log-likelihood: -7.922774791717529, Time taken: 5.8402 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 7\n",
      "EM converged in 3 iterations with log-likelihood: -7.0044660568237305, Time taken: 0.0076 seconds\n",
      "Adam (lr=10) converged in 35 iterations with log-likelihood: -16.614282608032227, Time taken: 0.3347 seconds\n",
      "Adam (lr=1.0) converged in 104 iterations with log-likelihood: -7.055148124694824, Time taken: 1.0996 seconds\n",
      "Adam (lr=0.1) converged in 54 iterations with log-likelihood: -7.008065700531006, Time taken: 0.5463 seconds\n",
      "Adam (lr=0.01) converged in 224 iterations with log-likelihood: -7.05768346786499, Time taken: 2.2659 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 8\n",
      "EM converged in 11 iterations with log-likelihood: -7.030754089355469, Time taken: 0.0194 seconds\n",
      "Adam (lr=10) converged in 214 iterations with log-likelihood: -19.538639068603516, Time taken: 2.3089 seconds\n",
      "Adam (lr=1.0) converged in 76 iterations with log-likelihood: -10.402925491333008, Time taken: 0.7627 seconds\n",
      "Adam (lr=0.1) converged in 679 iterations with log-likelihood: -7.037942886352539, Time taken: 10.6645 seconds\n",
      "Adam (lr=0.01) converged in 1361 iterations with log-likelihood: -7.858248710632324, Time taken: 19.4445 seconds\n",
      "\n",
      " Running Experiment 4 with 4 dimensions, 4 clusters, 1000 samples, and initialization: points with random state: 9\n",
      "EM converged in 13 iterations with log-likelihood: -7.066131114959717, Time taken: 0.0232 seconds\n",
      "Adam (lr=10) converged in 245 iterations with log-likelihood: -17.835094451904297, Time taken: 2.9266 seconds\n",
      "Adam (lr=1.0) converged in 59 iterations with log-likelihood: -9.01900863647461, Time taken: 0.8108 seconds\n",
      "Adam (lr=0.1) converged in 500 iterations with log-likelihood: -7.081212043762207, Time taken: 7.4713 seconds\n",
      "Adam (lr=0.01) converged in 534 iterations with log-likelihood: -8.288016319274902, Time taken: 7.2024 seconds\n",
      "Results saved to gmm_experiment_random_vs_points_initialization.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 10 random states\n",
    "random_states = np.arange(10)\n",
    "max_iter = 10001\n",
    "learning_rates = [10, 1., 1e-1, 1e-2]\n",
    "tol = 1e-4\n",
    "\n",
    "# Only using random and points initialization methods\n",
    "initialization_strategies = ['random', 'points']\n",
    "\n",
    "# Function to run a single experiment (comparing EM and Adam-based GMM with different learning rates and initializations)\n",
    "def run_experiment_with_init(X_tensor, y_tensor, dim, size, n_components, experiment_name, init_method, random_state, learning_rates=learning_rates):\n",
    "    results = []\n",
    "    print(f\"\\n Running {experiment_name} with {dim} dimensions, {n_components} clusters, {size} samples, and initialization: {init_method} with random state: {random_state}\")\n",
    "    \n",
    "    # Time the EM-based GMM\n",
    "    start_time = time.time()\n",
    "    gmm_em = GaussianMixture(\n",
    "        n_features=dim,\n",
    "        n_components=n_components,\n",
    "        covariance_type=covariance_type,\n",
    "        max_iter=max_iter,\n",
    "        init_params=init_method,\n",
    "        reg_covar=reg_covar,\n",
    "        random_state=random_state,\n",
    "        tol=tol\n",
    "    )\n",
    "    gmm_em.fit(X_tensor)\n",
    "    end_time = time.time()\n",
    "    time_taken_em = end_time - start_time\n",
    "\n",
    "    results_em = gmm_em.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "    print(f\"EM converged in {gmm_em.n_iter_} iterations with log-likelihood: {gmm_em.lower_bound_}, Time taken: {time_taken_em:.4f} seconds\")\n",
    "\n",
    "    # Collect EM results\n",
    "    results.append({\n",
    "        'experiment': experiment_name,\n",
    "        'method': 'EM',\n",
    "        'n_features': dim,\n",
    "        'n_clusters': n_components,\n",
    "        'data_size': size,\n",
    "        'learning_rate': None,\n",
    "        'iterations': gmm_em.n_iter_,\n",
    "        'log_likelihood': gmm_em.lower_bound_,\n",
    "        'time_taken': time_taken_em,\n",
    "        'init_method': init_method,\n",
    "        **results_em\n",
    "    })\n",
    "\n",
    "    # Run Adam-based GMM with different learning rates\n",
    "    for lr in learning_rates:\n",
    "        start_time = time.time()\n",
    "        gmm_adam = GaussianMixtureAdam(\n",
    "            n_features=dim,\n",
    "            n_components=n_components,\n",
    "            covariance_type=covariance_type,\n",
    "            max_iter=max_iter,\n",
    "            learning_rate=lr,\n",
    "            init_params=init_method,\n",
    "            reg_covar=reg_covar,\n",
    "            random_state=random_state,\n",
    "            tol=tol\n",
    "        )\n",
    "        gmm_adam.fit(X_tensor)\n",
    "        end_time = time.time()\n",
    "        time_taken_adam = end_time - start_time\n",
    "\n",
    "        results_adam = gmm_adam.evaluate_clustering(X_tensor, true_labels=y_tensor)\n",
    "        print(f\"Adam (lr={lr}) converged in {gmm_adam.n_iter_} iterations with log-likelihood: {gmm_adam.lower_bound_}, Time taken: {time_taken_adam:.4f} seconds\")\n",
    "\n",
    "        # Collect Adam results\n",
    "        results.append({\n",
    "            'experiment': experiment_name,\n",
    "            'method': 'Adam',\n",
    "            'n_features': dim,\n",
    "            'n_clusters': n_components,\n",
    "            'data_size': size,\n",
    "            'learning_rate': lr,\n",
    "            'iterations': gmm_adam.n_iter_,\n",
    "            'log_likelihood': gmm_adam.lower_bound_,\n",
    "            'time_taken': time_taken_adam,\n",
    "            'init_method': init_method,\n",
    "            **results_adam\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Experiment 4: Fixed clusters, fixed dimensions, and varying initializations\n",
    "experiment_results = []\n",
    "for init_method in initialization_strategies:\n",
    "    for random_state in random_states:\n",
    "        X_tensor = generate_data(1000, 4, centers_4d, random_state).to(device)\n",
    "        y_tensor = torch.cat([torch.zeros(250), torch.ones(250), 2 * torch.ones(250), 3 * torch.ones(250)]).long()\n",
    "        df = run_experiment_with_init(X_tensor, y_tensor, 4, 1000, 4, \"Experiment 4\", init_method, random_state)\n",
    "        experiment_results.append(df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results = pd.concat(experiment_results)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "file_name = 'gmm_experiment_random_vs_points_initialization.xlsx'\n",
    "folder = 'results'\n",
    "final_results.to_excel(f'{folder}/{file_name}', index=False)\n",
    "\n",
    "print(f\"Results saved to {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
