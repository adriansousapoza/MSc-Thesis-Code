{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    folder_path = '/content/drive/My Drive/Colab Notebooks/TorchGMM'\n",
    "    import os\n",
    "    files = os.listdir(folder_path)\n",
    "    print(files)\n",
    "    os.chdir(folder_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "folder = 'plots/MNIST_prior'\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Allocated memory: 38.703125 MB\n",
      "Reserved memory: 176.0 MB\n",
      "Total memory: 7836.25 MB\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.cm as cm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "if 'umap' not in globals():\n",
    "    !pip install umap-learn\n",
    "import umap\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import utils\n",
    "import utils.metrics\n",
    "import utils.gmm\n",
    "import utils.representation_layer\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.metrics)\n",
    "importlib.reload(utils.gmm)\n",
    "importlib.reload(utils.representation_layer)\n",
    "from utils.metrics import ClusteringMetrics\n",
    "from utils.representation_layer import RepresentationLayer\n",
    "from utils.gmm import GaussianMixture\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "    print('Using device:', torch.cuda.get_device_name(device))\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated(device)/1024**2} MB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved(device)/1024**2} MB\")\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(device).total_memory/1024**2} MB\")\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000\n",
      "Test dataset: 10000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Image size: 784\n"
     ]
    }
   ],
   "source": [
    "# Load a small subset of the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class IndexedDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "        return index, data, target\n",
    "\n",
    "indexed_train_dataset = IndexedDataset(train_dataset)\n",
    "indexed_test_dataset = IndexedDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(indexed_test_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "print('Train dataset:', len(indexed_train_dataset))\n",
    "print('Test dataset:', len(indexed_test_dataset))\n",
    "\n",
    "# print shape of an image\n",
    "print('Image shape:', train_dataset[0][0].shape)\n",
    "# print total number of pixels in an image\n",
    "print('Image size:', train_dataset[0][0].numel())\n",
    "\n",
    "all_labels = torch.tensor([label for _, _, label in indexed_train_dataset], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def fourier_feature_mapping(x, B):\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        B = B.to(x.device)\n",
    "        x_proj = (2. * np.pi * x) @ B.T\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, fourier_dim=None, scale=1.0, dropout=0.1, batch_norm=False):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        # Create the random Fourier feature matrix\n",
    "        self.B = None\n",
    "        if fourier_dim:\n",
    "            self.B = torch.randn(fourier_dim, latent_dim) * scale\n",
    "\n",
    "        # Linear layers construction\n",
    "        layers = [nn.Linear(2 * fourier_dim if fourier_dim else latent_dim, 128)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "        layers += [\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 128 * 7 * 7)\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(128 * 7 * 7))\n",
    "        layers += [\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ]\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(*layers)\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(128, 7, 7))\n",
    "\n",
    "        # Convolutional layers construction\n",
    "        conv_layers = [\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            conv_layers.append(nn.BatchNorm2d(64))\n",
    "        conv_layers += [\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            conv_layers.append(nn.BatchNorm2d(32))\n",
    "        conv_layers += [\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Fourier feature mapping if B is defined\n",
    "        if self.B is not None:\n",
    "            x = fourier_feature_mapping(x, self.B)\n",
    "        \n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss plot function\n",
    "def plot_loss(gmm_train_losses, gmm_test_losses, recon_train_losses, recon_test_losses, epoch):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # GMM Loss Plot\n",
    "    axes[0].plot(gmm_train_losses, label='Train Loss', color='red')\n",
    "    axes[0].plot(gmm_test_losses, label='Test Loss', color='green')\n",
    "    axes[0].set_title('GMM Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Reconstruction Loss Plot\n",
    "    axes[1].plot(recon_train_losses, label='Train Loss', color='red')\n",
    "    axes[1].plot(recon_test_losses, label='Test Loss', color='green')\n",
    "    axes[1].set_title('Reconstruction Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Total Loss Plot\n",
    "    total_train_losses = [gmm_train_losses[i] + recon_train_losses[i] for i in range(len(gmm_train_losses))]\n",
    "    total_test_losses = [gmm_test_losses[i] + recon_test_losses[i] for i in range(len(gmm_test_losses))]\n",
    "    axes[2].plot(total_train_losses, label='Train Loss', color='red')\n",
    "    axes[2].plot(total_test_losses, label='Test Loss', color='green')\n",
    "    axes[2].set_title('Total Loss')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.suptitle(f'Losses at Epoch {epoch}', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{folder}/losses_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_latent_space_visualizations(representations, labels, n_features, epoch, fraction=0.05, s=1, alpha=1, perplexity=20, gmm_means=None):\n",
    "    n_samples = int(fraction * representations.shape[0])\n",
    "    subset_indices = torch.randperm(representations.shape[0])[:n_samples]\n",
    "    representations_subset = representations[subset_indices].cpu().numpy()\n",
    "    labels_subset = labels[subset_indices].cpu().numpy()\n",
    "    \n",
    "    color_codes = ['#a50026', '#d73027', '#f46d43', '#fdae61', '#fee08b',\n",
    "                   '#d9ef8b', '#a6d96a', '#66bd63', '#1a9850', '#006837']\n",
    "    loc = 'upper right'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    if n_features == 2:\n",
    "        # Plot first vs second component directly\n",
    "        axes[0, 0].set_title('Latent Space (First vs Second Component)')\n",
    "        for digit in range(10):\n",
    "            digit_mask = labels_subset == digit\n",
    "            axes[0, 0].scatter(representations_subset[digit_mask, 0], representations_subset[digit_mask, 1],\n",
    "                               color=color_codes[digit], alpha=alpha, label=f'{digit}', s=s)\n",
    "        if gmm_means is not None:\n",
    "            axes[0, 0].scatter(gmm_means[:, 0], gmm_means[:, 1], marker='x', color='black', s=70, label='GMM Means')\n",
    "        axes[0, 0].legend(loc=loc)\n",
    "    else:\n",
    "        # PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        representations_pca = pca.fit_transform(representations_subset)\n",
    "        axes[0, 0].set_title('PCA')\n",
    "        for digit in range(10):\n",
    "            digit_mask = labels_subset == digit\n",
    "            axes[0, 0].scatter(representations_pca[digit_mask, 0], representations_pca[digit_mask, 1],\n",
    "                               color=color_codes[digit], alpha=alpha, label=f'{digit}', s=s)\n",
    "        if gmm_means is not None:\n",
    "            gmm_means_pca = pca.transform(gmm_means)\n",
    "            axes[0, 0].scatter(gmm_means_pca[:, 0], gmm_means_pca[:, 1], marker='x', color='black', s=70, label='GMM Means')\n",
    "        axes[0, 0].legend(loc=loc)\n",
    "\n",
    "    # Kernel PCA\n",
    "    kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "    representations_kpca = kpca.fit_transform(representations_subset)\n",
    "    for digit in range(10):\n",
    "        digit_mask = labels_subset == digit\n",
    "        axes[0, 1].scatter(representations_kpca[digit_mask, 0], representations_kpca[digit_mask, 1], \n",
    "                          color=color_codes[digit], alpha=alpha, label=f'{digit}', s=s)\n",
    "    axes[0, 1].set_title('Kernel PCA')\n",
    "    axes[0, 1].legend(loc=loc)\n",
    "    \n",
    "    # t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "    representations_tsne = tsne.fit_transform(representations_subset)\n",
    "    for digit in range(10):\n",
    "        digit_mask = labels_subset == digit\n",
    "        axes[1,0].scatter(representations_tsne[digit_mask, 0], representations_tsne[digit_mask, 1], \n",
    "                          color=color_codes[digit], alpha=alpha, label=f'{digit}', s=s)\n",
    "    axes[1,0].set_title('t-SNE')\n",
    "    axes[1,0].legend(loc=loc)\n",
    "    \n",
    "    # UMAP\n",
    "    umap_model = umap.UMAP(n_components=2)\n",
    "    representations_umap = umap_model.fit_transform(representations_subset)\n",
    "    for digit in range(10):\n",
    "        digit_mask = labels_subset == digit\n",
    "        axes[1,1].scatter(representations_umap[digit_mask, 0], representations_umap[digit_mask, 1], \n",
    "                          color=color_codes[digit], alpha=alpha, label=f'{digit}', s=s)\n",
    "    axes[1,1].set_title('UMAP')\n",
    "    axes[1,1].legend(loc=loc)\n",
    "    \n",
    "    plt.suptitle(f'Latent Space Visualizations at Epoch {epoch}', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{folder}/latent_space_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "def select_random_images(data_loader, device, num_images=10):\n",
    "    images = []\n",
    "    labels = []\n",
    "    indices = []\n",
    "    for batch_indices, x, batch_labels in data_loader:\n",
    "        for idx, img, label in zip(batch_indices, x, batch_labels):\n",
    "            images.append(img.to(device))\n",
    "            labels.append(label.to(device))\n",
    "            indices.append(idx.to(device))\n",
    "            if len(images) == num_images:\n",
    "                break\n",
    "        if len(images) == num_images:\n",
    "            break\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    indices = torch.stack(indices)\n",
    "    return images, labels, indices\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_images(images, title, epoch, max_cols=10, cmap='gray_r'):\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = [img.detach().cpu() for img in images]\n",
    "    elif isinstance(images, list):\n",
    "        images = [img.detach().cpu() if isinstance(img, torch.Tensor) else img for img in images]\n",
    "    else:\n",
    "        raise TypeError(\"Images should be a list or a torch.Tensor\")\n",
    "\n",
    "    print(f\"Plotting {len(images)} images.\")\n",
    "    n_images = len(images)\n",
    "    if n_images == 0:\n",
    "        print(\"No images to plot.\")\n",
    "        return\n",
    "    \n",
    "    if n_images <= 5:\n",
    "        n_cols = n_images\n",
    "    elif n_images <= 10:\n",
    "        n_cols = 5\n",
    "    else:\n",
    "        n_cols = min(max_cols, n_images)\n",
    "    n_rows = math.ceil(n_images / n_cols)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 1.5, n_rows * 1.5))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i in range(n_images):\n",
    "        ax = axes[i]\n",
    "        img = images[i].squeeze()\n",
    "        ax.imshow(img, cmap=cmap, interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Turn off any unused subplots\n",
    "    for i in range(n_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Epoch {epoch} - {title}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)  # Adjust to make room for suptitle\n",
    "    plt.savefig(f'{folder}/{title.replace(\" \", \"_\").lower()}_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_gmm_results(X, gmm, epoch):\n",
    "    X = X.detach().cpu().numpy()\n",
    "    log_probs = gmm.score_samples(torch.from_numpy(X).to(gmm.device)).cpu().numpy()\n",
    "    y_pred = gmm.predict(torch.from_numpy(X).to(gmm.device)).cpu().numpy()\n",
    "    \n",
    "    n_components = gmm.n_components\n",
    "    cmap = plt.get_cmap('tab20', n_components)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot 1: Per-Sample Log Likelihood\n",
    "    scatter = axs[0].scatter(X[:, 0], X[:, 1], c=log_probs, cmap='viridis', s=1, alpha=0.2)\n",
    "    axs[0].set_title('Per-Sample Log-Likelihood')\n",
    "    axs[0].set_xlabel('Feature 1')\n",
    "    axs[0].set_ylabel('Feature 2')\n",
    "    cbar = fig.colorbar(scatter, ax=axs[0])\n",
    "    cbar.set_label('Log-Likelihood')\n",
    "    \n",
    "    # Plot 2: Predicted Cluster Labels\n",
    "    scatter = axs[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap=cmap, s=1, alpha=0.2)\n",
    "    axs[1].set_title('Predicted Cluster Labels')\n",
    "    axs[1].set_xlabel('Feature 1')\n",
    "    axs[1].set_ylabel('Feature 2')\n",
    "    cbar = fig.colorbar(scatter, ax=axs[1], ticks=range(n_components))\n",
    "    cbar.set_label('Cluster Label')\n",
    "    \n",
    "    plt.suptitle(f'GMM Results at Epoch {epoch}', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{folder}/gmm_results_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gmm_generated_samples(gmm, epoch, n_samples=1000):\n",
    "    # Generate samples from the GMM\n",
    "    gmm_samples, gmm_labels = gmm.sample(n_samples=n_samples)\n",
    "    gmm_samples = gmm_samples.cpu().numpy()\n",
    "    gmm_labels = gmm_labels.cpu().numpy()\n",
    "    \n",
    "    n_components = gmm.n_components\n",
    "    cmap = plt.get_cmap('tab20', n_components)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Left Plot: Per-Sample Log Likelihood\n",
    "    log_probs = gmm.score_samples(torch.from_numpy(gmm_samples).to(gmm.device))\n",
    "    scatter = axs[0].scatter(gmm_samples[:, 0], gmm_samples[:, 1], c=log_probs.cpu().numpy(), cmap='viridis', s=10, alpha=.5)\n",
    "    axs[0].set_title('Per-Sample Log-Likelihood (Generated Samples)')\n",
    "    axs[0].set_xlabel('Feature 1')\n",
    "    axs[0].set_ylabel('Feature 2')\n",
    "    cbar = fig.colorbar(scatter, ax=axs[0])\n",
    "    cbar.set_label('Log-Likelihood')\n",
    "    \n",
    "    # Right Plot: Cluster Labels\n",
    "    scatter = axs[1].scatter(gmm_samples[:, 0], gmm_samples[:, 1], c=gmm_labels, cmap=cmap, s=10, alpha=.5)\n",
    "    axs[1].set_title('Generated Samples with Cluster Labels')\n",
    "    axs[1].set_xlabel('Feature 1')\n",
    "    axs[1].set_ylabel('Feature 2')\n",
    "    cbar = fig.colorbar(scatter, ax=axs[1], ticks=range(n_components))\n",
    "    cbar.set_label('Cluster Label')\n",
    "    \n",
    "    plt.suptitle(f'GMM Generated Samples at Epoch {epoch}', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{folder}/gmm_generated_samples_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_gmm_component_probabilities(X, gmm, title_prefix, epoch):\n",
    "    X = X.detach().cpu().numpy()\n",
    "    probs = gmm.predict_proba(torch.from_numpy(X).to(gmm.device)).detach().cpu().numpy()\n",
    "    n_components = gmm.n_components\n",
    "    n_cols = 2\n",
    "    n_rows = (n_components + 1) // n_cols\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for k, ax in enumerate(axs):\n",
    "        if k < n_components:\n",
    "            prob_k = probs[:, k]\n",
    "            scatter = ax.scatter(X[:, 0], X[:, 1], c=prob_k, cmap='viridis', s=1, alpha=0.2)\n",
    "            ax.set_title(f'Component {k+1} Probability')\n",
    "            ax.set_xlabel('Feature 1')\n",
    "            ax.set_ylabel('Feature 2')\n",
    "            cbar = fig.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Probability')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    fig.suptitle(f\"{title_prefix} Probability Distributions Across GMM Components at Epoch {epoch}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f'{folder}/gmm_component_probabilities_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n",
    "\n",
    "def get_covariance_matrix(gmm, n):\n",
    "    if gmm.covariance_type == 'full':\n",
    "        cov = gmm.covariances_[n].detach().cpu().numpy()\n",
    "    elif gmm.covariance_type == 'tied':\n",
    "        cov = gmm.covariances_.detach().cpu().numpy()\n",
    "    elif gmm.covariance_type == 'diag':\n",
    "        cov = np.diag(gmm.covariances_[n].detach().cpu().numpy())\n",
    "    elif gmm.covariance_type == 'spherical':\n",
    "        cov = np.eye(gmm.n_features) * gmm.covariances_[n].detach().cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported covariance type\")\n",
    "    return cov\n",
    "\n",
    "def plot_covariance_ellipse(ax, mean, cov, n_std=1, **kwargs):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals, vecs = vals[order], vecs[:, order]\n",
    "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    width, height = 2 * n_std * np.sqrt(vals)\n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "def plot_gmm_contours(gmm, X, epoch):\n",
    "    X = X.detach().cpu().numpy()\n",
    "    n_components = gmm.n_components\n",
    "    colors = plt.cm.get_cmap('tab20', n_components)\n",
    "    means = gmm.means_.detach().cpu().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # First Plot\n",
    "    ax = axs[0]\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='black', s=1, zorder=1, alpha=0.2, label='Representations')\n",
    "    ax.scatter(means[:, 0], means[:, 1], marker='x', color='red', s=50, zorder=2, label='GMM Means')\n",
    "    \n",
    "    for n in range(n_components):\n",
    "        mean = means[n]\n",
    "        cov = get_covariance_matrix(gmm, n)\n",
    "        plot_covariance_ellipse(ax, mean, cov, edgecolor='red', linestyle='--', zorder=3, facecolor='none')\n",
    "    \n",
    "    ax.set_title('GMM Means and Covariances')\n",
    "    ax.legend()\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    # Second Plot\n",
    "    ax = axs[1]\n",
    "    for n in range(n_components):\n",
    "        mean = means[n]\n",
    "        cov = get_covariance_matrix(gmm, n)\n",
    "        std = 1\n",
    "        plot_covariance_ellipse(ax, mean, cov, n_std=std, edgecolor=colors(n), facecolor=colors(n, alpha=0.5), zorder=2)\n",
    "    \n",
    "    ax.set_title('GMM Components')\n",
    "    ax.axis('equal')\n",
    "    plt.suptitle(f'Gaussian Mixture at Epoch {epoch}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{folder}/gmm_contours_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - GMM Loss: 3.1443 / 3.1387 - Recon Loss: 210.3138 / 207.0742 - GMM Convergence: True - TpE: 0:00:31 - RT: 2:36:06 - ETA: 2024-10-30 19:31:04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 295\u001b[0m\n\u001b[1;32m    291\u001b[0m testrep_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(test_rep\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mrep_lr, betas\u001b[38;5;241m=\u001b[39mrep_betas, amsgrad\u001b[38;5;241m=\u001b[39mrep_amsgrad)\n\u001b[1;32m    293\u001b[0m optimizers \u001b[38;5;241m=\u001b[39m [decoder_optimizer, rep_optimizer, testrep_optimizer]\n\u001b[0;32m--> 295\u001b[0m decoder, gmm, rep, test_rep \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_epoch_gmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit_gmm_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_gmm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_gmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 63\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, test_loader, model, gmm, rep, test_rep, optimizers, n_epochs, first_epoch_gmm, refit_gmm_interval, plot_step, plot_skip, lambda_gmm, n_images)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(gmm\u001b[38;5;241m.\u001b[39mlower_bound_)\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(recon_loss_x)\n\u001b[0;32m---> 63\u001b[0m     gmm_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlambda_gmm \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m recon_loss_x\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;241m+\u001b[39m gmm_error\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Downloads/Master Thesis/TorchGMM/utils/gmm.py:593\u001b[0m, in \u001b[0;36mGaussianMixture.score_samples\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    Compute the log-likelihood of each sample.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m     _, log_prob_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_prob_norm\n",
      "File \u001b[0;32m~/Downloads/Master Thesis/TorchGMM/utils/gmm.py:270\u001b[0m, in \u001b[0;36mGaussianMixture._e_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m diff \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     cholesky \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariances_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCholesky decomposition failed. Check covariance matrices. Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import timedelta\n",
    "\n",
    "def train_loop(train_loader, test_loader, model, gmm, rep, test_rep, optimizers, n_epochs, first_epoch_gmm=1, refit_gmm_interval=None, plot_step=None, plot_skip=0, lambda_gmm=1.0, n_images = 20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_optimizer, rep_optimizer, testrep_optimizer = optimizers\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    gmm_train_losses = []\n",
    "    gmm_test_losses = []\n",
    "    recon_train_losses = []\n",
    "    recon_test_losses = []\n",
    "\n",
    "    # Select a fixed set of random images for visualization\n",
    "    fixed_train_img, fixed_train_labels, fixed_train_indices = select_random_images(train_loader, device, num_images=n_components)\n",
    "    fixed_test_img, fixed_test_labels, fixed_test_indices = select_random_images(test_loader, device, num_images=n_components)\n",
    "\n",
    "    # Initialize previous GMM parameters as None\n",
    "    previous_means = None\n",
    "    previous_covariances = None\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train_losses.append(0)\n",
    "        test_losses.append(0)\n",
    "        gmm_train_losses.append(0)\n",
    "        gmm_test_losses.append(0)\n",
    "        recon_train_losses.append(0)\n",
    "        recon_test_losses.append(0)\n",
    "\n",
    "        if epoch > first_epoch_gmm and previous_means is not None and previous_covariances is not None:\n",
    "            gmm.set_mean_prior(previous_means, mean_precision_prior=1e-10)\n",
    "\n",
    "            scaling_factor = 0.01\n",
    "            degrees_of_freedom_prior = n_features + 2\n",
    "            gmm.set_covariance_prior(previous_covariances * scaling_factor, degrees_of_freedom_prior=degrees_of_freedom_prior)\n",
    "\n",
    "        # Training loop\n",
    "        rep_optimizer.zero_grad()\n",
    "        for i, (index, x, labels_batch) in enumerate(train_loader):\n",
    "            model_optimizer.zero_grad()\n",
    "\n",
    "            x, index, labels_batch = x.to(device), index.to(device), labels_batch.to(device)\n",
    "            z = rep(index)\n",
    "            y = model(z)\n",
    "            recon_loss_x = F.binary_cross_entropy(y, x, reduction='sum')\n",
    "\n",
    "            if epoch < first_epoch_gmm:\n",
    "                gmm_error = torch.tensor(0.0, device=device)\n",
    "                loss = recon_loss_x.clone()\n",
    "            else:\n",
    "                if i == 100000:\n",
    "                    print(min(gmm.score_samples(z)))\n",
    "                    print(max(gmm.score_samples(z)))\n",
    "                    print(gmm.score(z))\n",
    "                    print(torch.mean(gmm.score_samples(z)))\n",
    "                    print(torch.sum(gmm.score_samples(z)))\n",
    "                    print(gmm.lower_bound_)\n",
    "                    print(recon_loss_x)\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss_x.clone() + gmm_error.clone()\n",
    "\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "\n",
    "            train_losses[-1] += loss.item()\n",
    "            gmm_train_losses[-1] += gmm_error.item()\n",
    "            recon_train_losses[-1] += recon_loss_x.item()\n",
    "\n",
    "        rep_optimizer.step()\n",
    "\n",
    "        # Testing loop\n",
    "        testrep_optimizer.zero_grad()\n",
    "        for i, (index, x, _) in enumerate(test_loader):\n",
    "            x, index = x.to(device), index.to(device)\n",
    "            z = test_rep(index)\n",
    "            y = model(z)\n",
    "            recon_loss_x = F.binary_cross_entropy(y, x, reduction='sum')\n",
    "            if epoch < first_epoch_gmm:\n",
    "                gmm_error = torch.tensor(0.0, device=device)\n",
    "                loss = recon_loss_x.clone()\n",
    "            else:\n",
    "                gmm_error = -lambda_gmm * torch.sum(gmm.score_samples(z))\n",
    "                loss = recon_loss_x.clone() + gmm_error.clone()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            test_losses[-1] += loss.item()\n",
    "            gmm_test_losses[-1] += gmm_error.item()\n",
    "            recon_test_losses[-1] += recon_loss_x.item()\n",
    "\n",
    "        testrep_optimizer.step()\n",
    "\n",
    "        # Fit GMM with current representations\n",
    "        if epoch >= first_epoch_gmm:\n",
    "            with torch.no_grad():\n",
    "                representations = rep.z.detach().cpu()\n",
    "                if (refit_gmm_interval and epoch % refit_gmm_interval == 0) or epoch == n_epochs:\n",
    "                    gmm = GaussianMixture(\n",
    "                        n_features=n_features,\n",
    "                        n_components=n_components,\n",
    "                        covariance_type=covariance_type,\n",
    "                        init_params=init_params,\n",
    "                        device=device\n",
    "                    )\n",
    "                    if previous_means is not None and previous_covariances is not None:\n",
    "                        gmm.set_mean_prior(previous_means, mean_precision_prior=1e-10)\n",
    "                        scaling_factor = 0.01\n",
    "                        degrees_of_freedom_prior = n_features + 2\n",
    "                        gmm.set_covariance_prior(previous_covariances * scaling_factor, degrees_of_freedom_prior=degrees_of_freedom_prior)\n",
    "                    gmm.fit(representations, max_iter=10000)\n",
    "                else:\n",
    "                    gmm.fit(representations, max_iter=10, warm_start=True)\n",
    "            \n",
    "            if gmm.fitted_:\n",
    "                previous_means = gmm.means_.clone()\n",
    "                previous_covariances = gmm.covariances_.clone()\n",
    "\n",
    "        # Average the losses\n",
    "        train_losses[-1] /= len(train_loader.dataset)\n",
    "        test_losses[-1] /= len(test_loader.dataset)\n",
    "        gmm_train_losses[-1] /= len(train_loader.dataset)\n",
    "        gmm_test_losses[-1] /= len(test_loader.dataset)\n",
    "        recon_train_losses[-1] /= len(train_loader.dataset)\n",
    "        recon_test_losses[-1] /= len(test_loader.dataset)\n",
    "\n",
    "        # Timing calculations\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time_elapsed = time.time() - start_time\n",
    "        avg_epoch_time = total_time_elapsed / epoch\n",
    "        remaining_epochs = n_epochs - epoch\n",
    "        estimated_time_left = avg_epoch_time * remaining_epochs\n",
    "        estimated_finish_time = start_time + total_time_elapsed + estimated_time_left\n",
    "\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - GMM Loss: {gmm_train_losses[-1]:.4f} / {gmm_test_losses[-1]:.4f} - \"\n",
    "              f\"Recon Loss: {recon_train_losses[-1]:.4f} / {recon_test_losses[-1]:.4f} - \"\n",
    "              f\"GMM Convergence: {gmm.converged_} - \"\n",
    "              f\"TpE: {timedelta(seconds=int(epoch_time))} - \"\n",
    "              f\"RT: {timedelta(seconds=int(estimated_time_left))} - \"\n",
    "              f\"ETA: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(estimated_finish_time))}\")        \n",
    "\n",
    "        if (plot_step and epoch % plot_step == 0 and epoch > plot_skip) or epoch == n_epochs:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Plot the losses\n",
    "                plot_loss(gmm_train_losses, gmm_test_losses, recon_train_losses, recon_test_losses, epoch)\n",
    "\n",
    "                # Get representations and labels\n",
    "                representations = rep.z.detach()\n",
    "                test_representations = test_rep.z.detach()\n",
    "                labels = all_labels.cpu()\n",
    "                with torch.no_grad():\n",
    "                    recon_fixed_train = model(representations[:n_images]).squeeze().detach().cpu()\n",
    "                    recon_fixed_test = model(test_representations[:n_images]).squeeze().detach().cpu()\n",
    "\n",
    "                # Generate GMM means and samples\n",
    "                sampled, sampled_labels = gmm.sample(n_components)\n",
    "                sampled_imgs = [model(sample.unsqueeze(0)).squeeze().detach().cpu() for sample in sampled]\n",
    "                gmm_means_imgs = [model(mean.unsqueeze(0)).squeeze().detach().cpu() for mean in gmm.means_]\n",
    "\n",
    "                # Define titles and corresponding image sets\n",
    "                titles = [\n",
    "                    \"Cluster Centers\",\n",
    "                    \"New Samples\",\n",
    "                    \"Reconstructed Train Images\",\n",
    "                    \"Reconstructed Test Images\",\n",
    "                    \"Original Train Images\",\n",
    "                    \"Original Test Images\"\n",
    "                ]\n",
    "                image_sets = [\n",
    "                    gmm_means_imgs,\n",
    "                    sampled_imgs,\n",
    "                    recon_fixed_train[:n_components],  # Reconstructed Train Images\n",
    "                    recon_fixed_test[:n_components],   # Reconstructed Test Images\n",
    "                    fixed_train_img[:n_components],    # Original Train Images\n",
    "                    fixed_test_img[:n_components]      # Original Test Images\n",
    "                ]\n",
    "\n",
    "                # Call the plotting function for each set\n",
    "                for imgs, title in zip(image_sets, titles):\n",
    "                    plot_images(imgs, title, epoch, cmap='viridis')  # Use 'viridis' colormap\n",
    "\n",
    "                if epoch == n_epochs:\n",
    "                    plot_latent_space_visualizations(\n",
    "                        representations, labels, n_features, epoch, fraction=1, s=1, alpha=1,\n",
    "                        perplexity=20, gmm_means=gmm.means_.detach().cpu().numpy()\n",
    "                    )\n",
    "                else:\n",
    "                    plot_latent_space_visualizations(\n",
    "                        representations, labels, n_features, epoch, fraction=0.05, s=1, alpha=1,\n",
    "                        perplexity=20, gmm_means=gmm.means_.detach().cpu().numpy()\n",
    "                    )\n",
    "                \n",
    "                plot_gmm_results(representations, gmm, epoch)\n",
    "                plot_gmm_generated_samples(gmm, epoch)\n",
    "                plot_gmm_contours(gmm, representations, epoch)\n",
    "                \n",
    "                X = rep.z.detach()\n",
    "                true_labels = all_labels.cpu()\n",
    "\n",
    "                # Evaluate clustering\n",
    "                results = gmm.evaluate_clustering(X, true_labels=true_labels, metrics=metrics_list)\n",
    "\n",
    "                # Print metrics\n",
    "                for metric, score in results.items():\n",
    "                    if metric != 'confusion_matrix' and metric != 'classification_report':\n",
    "                        print(f\"{metric}: {score}\")\n",
    "\n",
    "                # Confusion matrix\n",
    "                cm = results['confusion_matrix']\n",
    "\n",
    "                # Plot confusion matrix\n",
    "                import seaborn as sns\n",
    "\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.xlabel('Predicted Labels')\n",
    "                plt.ylabel('True Labels')\n",
    "                plt.title('Confusion Matrix')\n",
    "                plt.savefig(f'{folder}/confusion_matrix_epoch_{epoch}.pdf', format='pdf', dpi=400)\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "            model.train()\n",
    "\n",
    "    model.eval()\n",
    "    print('Training completed.')\n",
    "    return model, gmm, rep, test_rep\n",
    "\n",
    "metrics_list = [\n",
    "    \"rand_score\",\n",
    "    \"adjusted_rand_score\",\n",
    "    \"mutual_info_score\",\n",
    "    \"normalized_mutual_info_score\",\n",
    "    \"adjusted_mutual_info_score\",\n",
    "    \"fowlkes_mallows_score\",\n",
    "    \"homogeneity_score\",\n",
    "    \"completeness_score\",\n",
    "    \"v_measure_score\",\n",
    "    \"purity_score\",\n",
    "    \"classification_report\",\n",
    "    \"confusion_matrix\",\n",
    "    \"bic_score\",\n",
    "    \"aic_score\",\n",
    "    # Exclude 'silhouette_score', 'davies_bouldin_index', 'calinski_harabasz_score', 'dunn_index'\n",
    "]\n",
    "\n",
    "# Initialize components\n",
    "n_features = 2\n",
    "n_components = 20\n",
    "epochs = 300\n",
    "plot_step = 50\n",
    "plot_skip = 0\n",
    "first_epoch_gmm = 1\n",
    "refit_gmm_interval = 50\n",
    "nsample = len(indexed_train_dataset)\n",
    "nsample_test = len(indexed_test_dataset)\n",
    "lambda_gmm = 1.0\n",
    "\n",
    "# Adam Parameters\n",
    "decoder_lr = 1e-2\n",
    "rep_lr = 1e-1\n",
    "decoder_weight_decay = 1e-5\n",
    "decoder_betas = (0.9, 0.999)\n",
    "rep_betas = (0.9, 0.999)\n",
    "decoder_amsgrad = True\n",
    "rep_amsgrad =False\n",
    "\n",
    "# GMM Parameters\n",
    "covariance_type = 'full'\n",
    "init_params = 'kmeans'\n",
    "    \n",
    "# Decoder Parameters\n",
    "dropout = 0.1\n",
    "batch_norm = False\n",
    "fourier_dim = None\n",
    "scale = 1\n",
    "\n",
    "gmm = GaussianMixture(n_features=n_features, n_components=n_components, covariance_type=covariance_type, init_params=init_params, device=device)\n",
    "\n",
    "rep = RepresentationLayer(values=torch.randn(size=(nsample, n_features)).to(device))\n",
    "test_rep = RepresentationLayer(values=torch.randn(size=(nsample_test, n_features)).to(device))\n",
    "\n",
    "decoder = ConvDecoder(latent_dim=n_features, fourier_dim=fourier_dim, scale=scale, dropout=dropout, batch_norm=batch_norm).to(device)\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=decoder_lr, betas=decoder_betas, amsgrad=decoder_amsgrad, weight_decay=decoder_weight_decay)\n",
    "rep_optimizer = torch.optim.Adam(rep.parameters(), lr=rep_lr, betas=rep_betas, amsgrad=rep_amsgrad)\n",
    "testrep_optimizer = torch.optim.Adam(test_rep.parameters(), lr=rep_lr, betas=rep_betas, amsgrad=rep_amsgrad)\n",
    "\n",
    "optimizers = [decoder_optimizer, rep_optimizer, testrep_optimizer]\n",
    "\n",
    "decoder, gmm, rep, test_rep = train_loop(train_loader, test_loader, decoder, gmm, rep, test_rep, optimizers, n_epochs=epochs, first_epoch_gmm=first_epoch_gmm, refit_gmm_interval=refit_gmm_interval, plot_step=plot_step, plot_skip=plot_skip, lambda_gmm=lambda_gmm, n_images=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
